[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this section we will employ dimensionality reduction techniques and clustering methods. These will include principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), K-Means, DBSCAN, and hierarchical clustering. The objective is to explore the different methods, investigate the dataset further, and understand any consistent patterns that may help answer our guiding Data Science questions.."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "On this page, we are going to perform exploratory data anlysis (EDA) in order to get an initial sense of what the dataset looks like and understand what methods we can take moving forward. We will look at the main variables of the datasets, identifying patterns and potential issues along the way. We will summarize key characteristic and potenital anomalies. The process will be broken up into the following stages: Univariate Analysis, Bivariate and Multivariate Analysis, Statistical Insights, and Concluding Remarks.\nTo understand the followings visualizations and discussion, it is pertinent to first delve into the underlying data. As we have discussed in the data collection section of the website, this data was scraped from Yelp.com utilizing an API. The data is contains information about specific restaurants on the Yelp website, therefore, the unit of observation is a particular restaurant. All the restaurants are located in the DC area and the data contains a total of 737 individual and unique restaurants.\nThe end goal of this EDA section is to better understand the underlying data and determine if we are able to move forward with answering our key data science questions. We will now begin the investigation of the yelp dataset."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Introduction and Motivation\nThe purpose of this page and the code below is to clean and tidy the collected data. This is necessary to perform before working with the code in order to avoid errors and inconsistencies. Fully cleaning the data before performing exploratory data analysis and statistical methods will ensure that we are working with accurate information and will also improve efficiency of future work due to having to deal with a fewer number of minor errors. Our conclusion and insights could potential be incorrect or not as accurate as possible if the data is not properly cleaned.\nFor our purposes the data cleaning process will entail the following steps. Firstly, we will identify and handle pertinent missing data. Then, outliers will be identified, addressed, and visualized to understand how they are impacting the dataset and individuals distributions. The data types will be understood and corrected to fit the needs of our future analysis. Finally, the dataset will be inspected for troublesome skewness and will later be normalized and scaled in the supervised learning portion of this work (see Supervised Learning tab for this work).\nThe cleaned dataset will be outputted to use for future analysis, allowing for smooth and accurate work moving forward.\n\n\nCode and Explanations\n\nWorking with the Scraped Yelp Data\nThis section is to clean and combine the yelp datasets in order to later combine it with the zipcode income data\n\n# importing all the necessary libraries for data cleaning\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\n\n\n# reading-in the one of the Yelp datasets\ndf = pd.read_csv(\"../../data/raw-data/df_bars1.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nname\ncuisine\nprice_range\nrating\nreview_count\nneighborhoods\nlatitude\nlongitude\nzip_code\n\n\n\n\n0\n0\nJane Jane\nCocktail Bars\n$$\n4.5\n131\nNaN\n38.912817\n-77.03174\n20009\n\n\n1\n1\nCODE RED\nCocktail Bars\n$$\n4.6\n114\nNaN\n38.921680\n-77.04270\n20009\n\n\n2\n2\nThe Crown & Crow\nBars\n$$\n4.5\n149\nNaN\n38.907830\n-77.03162\n20005\n\n\n3\n3\nThe Alchemist DC\nSpeakeasies\nNaN\n4.2\n82\nNaN\n38.916810\n-77.03099\n20009\n\n\n4\n4\nAlegria\nBars\nNaN\n4.4\n29\nNaN\n38.922076\n-76.99657\n20002\n\n\n\n\n\n\n\n\n# setting the data directory\ndata_dir = \"../../data/raw-data/\"\n\n# listing the files in the directory\nfile_names = os.listdir(data_dir)\n\n# creating the paths to each of the files\nfile_path = [os.path.join(data_dir, file) for file in file_names if file.endswith(\".csv\")]\n\n# creating one large file of all combined\ndf1 = pd.concat(map(pd.read_csv, file_path), ignore_index=True)\n\n# saving the combined file of all the yelp pages\ndf1.to_csv(\"../../data/processed-data/combined_yelp.csv\")\n\n\ndf1 = df1.drop(['neighborhoods'], axis=1)\ndf1 = df1.drop(['Unnamed: 0'], axis=1)\ndf1 = df1.fillna(0)\ndf1.head()\n\n\n\n\n\n\n\n\nname\ncuisine\nprice_range\nrating\nreview_count\nlatitude\nlongitude\nzip_code\nLabel (Grouping)\nDistrict of Columbia!!Estimate\n...\nZCTA5 20551!!Estimate\nZCTA5 20551!!Margin of Error\nZCTA5 20560!!Estimate\nZCTA5 20560!!Margin of Error\nZCTA5 20565!!Estimate\nZCTA5 20565!!Margin of Error\nZCTA5 20566!!Estimate\nZCTA5 20566!!Margin of Error\nZCTA5 20591!!Estimate\nZCTA5 20591!!Margin of Error\n\n\n\n\n0\nSimona cafe\nCoffee & Tea\n0\n4.2\n31.0\n38.906938\n-77.007667\n20002.0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nGregorys Coffee\nCoffee & Tea\n$$\n4.1\n118.0\n38.904302\n-77.039933\n20036.0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nFor Five Coffee Roasters\nCoffee & Tea\n0\n3.4\n28.0\n38.901666\n-77.045230\n20006.0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nGrafika Coffee\nCoffee & Tea\n0\n5.0\n1.0\n38.907515\n-77.007494\n20036.0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nGregorys Coffee\nCoffee & Tea\n$$\n3.6\n112.0\n38.895950\n-77.021780\n20004.0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 125 columns\n\n\n\n\n# outputting the combined yelp data\ndf1.to_csv(\"../../data/processed-data/combined_yelp.csv\")\n\n\n\nWorking with the Zip Code Income Data\nHere we are cleaning the zipcode and median income dataset to make it ready for merging.\n\n# reading in the zipcode dataset\nzip_code = pd.read_csv(\"../../data/raw-data/ACSDT5Y2022.B19013-2024-11-29T205354.csv\")\nzip_code = zip_code.T # transposing it\n# resetting the index\nzip_code.reset_index(inplace=True)\n# renaming the variables\nzip_code.columns = [\"Category\", \"Value\"]\n# displaying the dataframe\nprint(zip_code)\n\n                                  Category  \\\n0                         Label (Grouping)   \n1           District of Columbia!!Estimate   \n2    District of Columbia!!Margin of Error   \n3                    ZCTA5 20001!!Estimate   \n4             ZCTA5 20001!!Margin of Error   \n..                                     ...   \n112           ZCTA5 20565!!Margin of Error   \n113                  ZCTA5 20566!!Estimate   \n114           ZCTA5 20566!!Margin of Error   \n115                  ZCTA5 20591!!Estimate   \n116           ZCTA5 20591!!Margin of Error   \n\n                                                 Value  \n0    Median household income in the past 12 months ...  \n1                                              101,722  \n2                                               ±1,569  \n3                                              133,211  \n4                                               ±9,474  \n..                                                 ...  \n112                                                 **  \n113                                                  -  \n114                                                 **  \n115                                                  -  \n116                                                 **  \n\n[117 rows x 2 columns]\n\n\n\n# grabbing just the Median Income information\nzip_code = zip_code[zip_code[\"Category\"].str.contains(\"Margin of Error\") == False]\nzip_code.head()\n\n\n\n\n\n\n\n\nCategory\nValue\n\n\n\n\n0\nLabel (Grouping)\nMedian household income in the past 12 months ...\n\n\n1\nDistrict of Columbia!!Estimate\n101,722\n\n\n3\nZCTA5 20001!!Estimate\n133,211\n\n\n5\nZCTA5 20002!!Estimate\n107,130\n\n\n7\nZCTA5 20003!!Estimate\n155,054\n\n\n\n\n\n\n\n\n# extractng the zipcodes\nzip_code['ZIP_Code'] = zip_code['Category'].str.extract(r'ZCTA5 (\\d{5})')\nzip_code.head()\n\n\n\n\n\n\n\n\nCategory\nValue\nZIP_Code\n\n\n\n\n0\nLabel (Grouping)\nMedian household income in the past 12 months ...\nNaN\n\n\n1\nDistrict of Columbia!!Estimate\n101,722\nNaN\n\n\n3\nZCTA5 20001!!Estimate\n133,211\n20001\n\n\n5\nZCTA5 20002!!Estimate\n107,130\n20002\n\n\n7\nZCTA5 20003!!Estimate\n155,054\n20003\n\n\n\n\n\n\n\n\n#dropping first column \nlist(zip_code)\nzip_code = zip_code.drop('Category', axis=1)\nzip_code.head()\n\n\n\n\n\n\n\n\nValue\nZIP_Code\n\n\n\n\n0\nMedian household income in the past 12 months ...\nNaN\n\n\n1\n101,722\nNaN\n\n\n3\n133,211\n20001\n\n\n5\n107,130\n20002\n\n\n7\n155,054\n20003\n\n\n\n\n\n\n\n\n# dropping the first two rows that contain words\nzip_code = zip_code.iloc[2:, :]\nzip_code.head()\n\n\n\n\n\n\n\n\nValue\nZIP_Code\n\n\n\n\n3\n133,211\n20001\n\n\n5\n107,130\n20002\n\n\n7\n155,054\n20003\n\n\n9\n152,955\n20004\n\n\n11\n109,147\n20005\n\n\n\n\n\n\n\n\n# renaming the columns for clarity\nzip_code.columns = ['Median_Income', 'zip_code']\nzip_code\n\n\n\n\n\n\n\n\nMedian_Income\nzip_code\n\n\n\n\n3\n133,211\n20001\n\n\n5\n107,130\n20002\n\n\n7\n155,054\n20003\n\n\n9\n152,955\n20004\n\n\n11\n109,147\n20005\n\n\n13\n34,352\n20006\n\n\n15\n145,048\n20007\n\n\n17\n123,134\n20008\n\n\n19\n132,374\n20009\n\n\n21\n106,560\n20010\n\n\n23\n97,327\n20011\n\n\n25\n110,375\n20012\n\n\n27\n235,511\n20015\n\n\n29\n169,489\n20016\n\n\n31\n97,507\n20017\n\n\n33\n87,552\n20018\n\n\n35\n53,394\n20019\n\n\n37\n48,106\n20020\n\n\n39\n97,694\n20024\n\n\n41\n47,871\n20032\n\n\n43\n106,930\n20036\n\n\n45\n94,820\n20037\n\n\n47\n-\n20045\n\n\n49\n-\n20052\n\n\n51\n-\n20057\n\n\n53\n-\n20059\n\n\n55\n-\n20064\n\n\n57\n-\n20204\n\n\n59\n-\n20220\n\n\n61\n-\n20230\n\n\n63\n-\n20240\n\n\n65\n-\n20245\n\n\n67\n-\n20250\n\n\n69\n-\n20260\n\n\n71\n-\n20317\n\n\n73\n-\n20319\n\n\n75\n-\n20373\n\n\n77\n-\n20388\n\n\n79\n-\n20390\n\n\n81\n-\n20408\n\n\n83\n-\n20415\n\n\n85\n-\n20418\n\n\n87\n-\n20422\n\n\n89\n-\n20427\n\n\n91\n-\n20431\n\n\n93\n-\n20510\n\n\n95\n-\n20515\n\n\n97\n-\n20520\n\n\n99\n-\n20530\n\n\n101\n-\n20535\n\n\n103\n-\n20540\n\n\n105\n-\n20542\n\n\n107\n-\n20551\n\n\n109\n-\n20560\n\n\n111\n-\n20565\n\n\n113\n-\n20566\n\n\n115\n-\n20591\n\n\n\n\n\n\n\n\n# keeping only the rows with observations (0 to 45)\nzip_code = zip_code.iloc[:46]\nzip_code = zip_code[zip_code[\"Median_Income\"].str.contains(\"-\") == False]\nprint(zip_code)\n\n   Median_Income zip_code\n3        133,211    20001\n5        107,130    20002\n7        155,054    20003\n9        152,955    20004\n11       109,147    20005\n13        34,352    20006\n15       145,048    20007\n17       123,134    20008\n19       132,374    20009\n21       106,560    20010\n23        97,327    20011\n25       110,375    20012\n27       235,511    20015\n29       169,489    20016\n31        97,507    20017\n33        87,552    20018\n35        53,394    20019\n37        48,106    20020\n39        97,694    20024\n41        47,871    20032\n43       106,930    20036\n45        94,820    20037\n\n\n\nprint(zip_code)\n# outputting the cleaned zip code and median income data\nzip_code.to_csv('../../data/processed-data/zip_code.csv')\n\n   Median_Income zip_code\n3        133,211    20001\n5        107,130    20002\n7        155,054    20003\n9        152,955    20004\n11       109,147    20005\n13        34,352    20006\n15       145,048    20007\n17       123,134    20008\n19       132,374    20009\n21       106,560    20010\n23        97,327    20011\n25       110,375    20012\n27       235,511    20015\n29       169,489    20016\n31        97,507    20017\n33        87,552    20018\n35        53,394    20019\n37        48,106    20020\n39        97,694    20024\n41        47,871    20032\n43       106,930    20036\n45        94,820    20037\n\n\n\n\nMerging Yelp and Zipcode/Income\nWe will now merge the two cleaned dataset into one.\n\n# reading and checking both datasets\ndf1 = pd.read_csv(\"../../data/processed-data/zip_code.csv\")\ndf2 = pd.read_csv(\"../../data/processed-data/combined_yelp.csv\")\nprint(df1)\nprint(df2)\n\n    Unnamed: 0 Median_Income  zip_code\n0            3       133,211     20001\n1            5       107,130     20002\n2            7       155,054     20003\n3            9       152,955     20004\n4           11       109,147     20005\n5           13        34,352     20006\n6           15       145,048     20007\n7           17       123,134     20008\n8           19       132,374     20009\n9           21       106,560     20010\n10          23        97,327     20011\n11          25       110,375     20012\n12          27       235,511     20015\n13          29       169,489     20016\n14          31        97,507     20017\n15          33        87,552     20018\n16          35        53,394     20019\n17          37        48,106     20020\n18          39        97,694     20024\n19          41        47,871     20032\n20          43       106,930     20036\n21          45        94,820     20037\n     Unnamed: 0                      name          cuisine price_range  \\\n0             0               Simona cafe     Coffee & Tea           0   \n1             1           Gregorys Coffee     Coffee & Tea          $$   \n2             2  For Five Coffee Roasters     Coffee & Tea           0   \n3             3            Grafika Coffee     Coffee & Tea           0   \n4             4           Gregorys Coffee     Coffee & Tea          $$   \n..          ...                       ...              ...         ...   \n746         746                Fight Club             Bars          $$   \n747         747                 Stable DC  Modern European          $$   \n748         748                  Tamashaa           Indian           0   \n749         749          The Park at 14th             Bars          $$   \n750         750                  Emissary     Coffee & Tea          $$   \n\n     rating  review_count   latitude  longitude  zip_code Label (Grouping)  \\\n0       4.2          31.0  38.906938 -77.007667   20002.0                0   \n1       4.1         118.0  38.904302 -77.039933   20036.0                0   \n2       3.4          28.0  38.901666 -77.045230   20006.0                0   \n3       5.0           1.0  38.907515 -77.007494   20036.0                0   \n4       3.6         112.0  38.895950 -77.021780   20004.0                0   \n..      ...           ...        ...        ...       ...              ...   \n746     4.0         470.0  38.884784 -76.997326   20003.0                0   \n747     4.6         348.0  38.900399 -76.987348   20002.0                0   \n748     4.4          47.0  38.929705 -77.032287   20010.0                0   \n749     3.7        1297.0  38.901922 -77.032275   20005.0                0   \n750     4.0         537.0  38.909510 -77.046380   20036.0                0   \n\n     ... ZCTA5 20551!!Estimate ZCTA5 20551!!Margin of Error  \\\n0    ...                     0                            0   \n1    ...                     0                            0   \n2    ...                     0                            0   \n3    ...                     0                            0   \n4    ...                     0                            0   \n..   ...                   ...                          ...   \n746  ...                     0                            0   \n747  ...                     0                            0   \n748  ...                     0                            0   \n749  ...                     0                            0   \n750  ...                     0                            0   \n\n    ZCTA5 20560!!Estimate ZCTA5 20560!!Margin of Error ZCTA5 20565!!Estimate  \\\n0                       0                            0                     0   \n1                       0                            0                     0   \n2                       0                            0                     0   \n3                       0                            0                     0   \n4                       0                            0                     0   \n..                    ...                          ...                   ...   \n746                     0                            0                     0   \n747                     0                            0                     0   \n748                     0                            0                     0   \n749                     0                            0                     0   \n750                     0                            0                     0   \n\n    ZCTA5 20565!!Margin of Error ZCTA5 20566!!Estimate  \\\n0                              0                     0   \n1                              0                     0   \n2                              0                     0   \n3                              0                     0   \n4                              0                     0   \n..                           ...                   ...   \n746                            0                     0   \n747                            0                     0   \n748                            0                     0   \n749                            0                     0   \n750                            0                     0   \n\n    ZCTA5 20566!!Margin of Error ZCTA5 20591!!Estimate  \\\n0                              0                     0   \n1                              0                     0   \n2                              0                     0   \n3                              0                     0   \n4                              0                     0   \n..                           ...                   ...   \n746                            0                     0   \n747                            0                     0   \n748                            0                     0   \n749                            0                     0   \n750                            0                     0   \n\n    ZCTA5 20591!!Margin of Error  \n0                              0  \n1                              0  \n2                              0  \n3                              0  \n4                              0  \n..                           ...  \n746                            0  \n747                            0  \n748                            0  \n749                            0  \n750                            0  \n\n[751 rows x 126 columns]\n\n\n\n# outer merging the datsets\nmerge_df = pd.merge(df1,df2,on =\"zip_code\",how=\"outer\")\nmerge_df = merge_df.fillna(0) # fill missing with zeros\n\n\n# dropping the unnecessary columns\nmerge_df = merge_df.drop(['Unnamed: 0_x','Unnamed: 0_y'], axis=1)\nmerge_df.head()\n\n\n\n\n\n\n\n\nMedian_Income\nzip_code\nname\ncuisine\nprice_range\nrating\nreview_count\nlatitude\nlongitude\nLabel (Grouping)\n...\nZCTA5 20551!!Estimate\nZCTA5 20551!!Margin of Error\nZCTA5 20560!!Estimate\nZCTA5 20560!!Margin of Error\nZCTA5 20565!!Estimate\nZCTA5 20565!!Margin of Error\nZCTA5 20566!!Estimate\nZCTA5 20566!!Margin of Error\nZCTA5 20591!!Estimate\nZCTA5 20591!!Margin of Error\n\n\n\n\n0\n0\n0.0\n0\n0\n0\n0.0\n0.0\n0.000000\n0.000000\nMedian household income in the past 12 months ...\n...\n-\n**\n-\n**\n-\n**\n-\n**\n-\n**\n\n\n1\n0\n20000.0\nLapop\nCoffee & Tea\n$$\n4.1\n38.0\n38.921329\n-77.043835\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n20000.0\nLapop\nCoffee & Tea\n$$\n4.1\n38.0\n38.921329\n-77.043835\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n133,211\n20001.0\nCompass Coffee\nCoffee & Tea\n$$\n4.1\n92.0\n38.916256\n-77.022773\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n133,211\n20001.0\nMarianne’s by DC Central Kitchen\nCafes\n0\n4.6\n17.0\n38.898720\n-77.024770\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 126 columns\n\n\n\n\n# saving this merged dataset\nmerge_df.to_csv('../../data/processed-data/yelp_zip.csv')\n\n\n# reading in the combined dataset and keeping necessary columns\nyelp_zip = pd.read_csv(\"../../data/processed-data/yelp_zip.csv\")\nyelp_zip = yelp_zip[['Median_Income', 'zip_code', 'name', 'cuisine', 'price_range', 'rating', 'review_count', 'latitude', 'longitude']]\nprint(yelp_zip.head(5))\n\n  Median_Income  zip_code                              name       cuisine  \\\n0             0       0.0                                 0             0   \n1             0   20000.0                             Lapop  Coffee & Tea   \n2             0   20000.0                             Lapop  Coffee & Tea   \n3       133,211   20001.0                    Compass Coffee  Coffee & Tea   \n4       133,211   20001.0  Marianne’s by DC Central Kitchen         Cafes   \n\n  price_range  rating  review_count   latitude  longitude  \n0           0     0.0           0.0   0.000000   0.000000  \n1          $$     4.1          38.0  38.921329 -77.043835  \n2          $$     4.1          38.0  38.921329 -77.043835  \n3          $$     4.1          92.0  38.916256 -77.022773  \n4           0     4.6          17.0  38.898720 -77.024770  \n\n\n\n\nData Type Correction and Formatting:\nIn this section, variables are numerized and a price variable is created based on the Yelp price ranges given on their website. The cuisine variable will be combined into fewer categories for use in later analysis.\n\n# check that the variables are correct type\nprint(yelp_zip.dtypes)\n\n# making median income numeric\nyelp_zip['Median_Income'] = yelp_zip['Median_Income'].str.replace(\",\", \"\")\nyelp_zip['Median_Income'] = pd.to_numeric(yelp_zip['Median_Income'])\n\nprint(yelp_zip.dtypes)\n\nMedian_Income     object\nzip_code         float64\nname              object\ncuisine           object\nprice_range       object\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\ndtype: object\nMedian_Income      int64\nzip_code         float64\nname              object\ncuisine           object\nprice_range       object\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\ndtype: object\n\n\n\n# encoding the price_range variable (number is the amount of dollar signs)\n# just needed something numeric for price\ndef price_conversion(price):\n    if price == \"$\":\n        return 1\n    elif price == \"$$\":\n        return 2\n    elif price == \"$$$\":\n        return 3\n    elif price == \"$$$$\":\n        return 4\n    else:\n        return 0\n    \nyelp_zip['price_range'] = yelp_zip['price_range'].apply(price_conversion)\n\n# There are a good amount of zeros, so will drop these:\nyelp_zip = yelp_zip[yelp_zip['price_range'] != 0]\n\n# numerizing the price_range variable:\n# used the avergae of the ranges given by Yelp\nprice_range_values = yelp_zip['price_range'].unique()\ndef price_conversion(price):\n    if price &lt;= 1:\n        return 5\n    elif price &lt;= 2:\n        return 20.5\n    elif price &lt;= 3:\n        return 45.5\n    elif price &lt;= 4:\n        return 61\n    else:\n        return 0\nyelp_zip['price'] = yelp_zip['price_range'].apply(price_conversion)\n\nprint(yelp_zip.head(5))\n\n   Median_Income  zip_code                        name       cuisine  \\\n1              0   20000.0                       Lapop  Coffee & Tea   \n2              0   20000.0                       Lapop  Coffee & Tea   \n3         133211   20001.0              Compass Coffee  Coffee & Tea   \n5         133211   20001.0  Sankofa Video Books & Cafe    Bookstores   \n6         133211   20001.0           La Colombe Coffee  Coffee & Tea   \n\n   price_range  rating  review_count   latitude  longitude  price  \n1            2     4.1          38.0  38.921329 -77.043835   20.5  \n2            2     4.1          38.0  38.921329 -77.043835   20.5  \n3            2     4.1          92.0  38.916256 -77.022773   20.5  \n5            1     4.5         167.0  38.925561 -77.023150    5.0  \n6            2     4.0         303.0  38.901051 -77.020103   20.5  \n\n\n\n# dropping cuisines that are zero\nyelp_zip = yelp_zip[yelp_zip['cuisine'] != \"0\"]\n\n# investigating the cuisine variable\ncuisine_values = yelp_zip['cuisine'].unique()\nprint(cuisine_values)\n\n['Coffee & Tea' 'Bookstores' 'Bakeries' 'Cafes' 'Breakfast & Brunch'\n 'New American' 'Bars' 'Italian' 'Mexican' 'Peruvian' 'French' 'Pubs'\n 'Chinese' 'Greek' 'Spanish' 'Thai' 'Latin American' 'Japanese' 'Ramen'\n 'Persian/Iranian' 'Burgers' 'Korean' 'Sandwiches' 'Sports Bars'\n 'American' 'Delis' 'Puerto Rican' 'Modern European' 'Salvadoran'\n 'Asian Fusion' 'Cocktail Bars' 'Irish' 'Dive Bars' 'Beer Gardens'\n 'Dance Clubs' 'Pool Halls' 'Music Venues' 'Wine Bars' 'Lounges'\n 'Bubble Tea' 'Pizza' 'Trinidadian' 'Laotian' 'Sushi Bars' 'Mediterranean'\n 'Steakhouses' 'Cultural Center' 'Seafood' 'Distilleries' 'Breweries'\n 'Herbs & Spices' 'Indian' 'Southern' 'British' 'Mini Golf'\n 'Juice Bars & Smoothies' 'Coffee Roasteries' 'Tapas/Small Plates'\n 'Cantonese' 'Bagels' 'Venezuelan' 'Whiskey Bars' 'Gay Bars' 'Vietnamese'\n 'Creperies' 'Ukrainian' 'Noodles' 'Belgian' 'Jazz & Blues' 'Dim Sum'\n 'Scandinavian' 'Desserts' 'Vinyl Records' 'Barbeque' 'Brasseries'\n 'Moroccan' 'Afghan' 'Beer Bar' 'Tiki Bars' 'Filipino'\n 'Himalayan/Nepalese' 'Tacos' 'Salad' 'Lebanese' 'Fish & Chips' 'Falafel'\n 'Szechuan' 'Egyptian' 'German' 'Shared Office Spaces' 'Pan Asian']\n\n\n\n# grouping cuisine types together to make fewer categories\ndef categorize_cuisine(cuisine):\n  cafes = ['Coffee & Tea', 'Cafes', 'Coffee Roasteries', 'Bubble Tea', 'Juice Bars & Smoothies', 'Themed Cafes', 'Art Classes', 'Bookstores', 'Vinyl Records', 'Pet Adoption', 'Banks & Credit Unions', 'Shared Office Spaces', 'Hong Kong Style Cafe', 'Bakeries', 'Desserts', 'Chocolatiers & Shops', 'Creperies', 'Herbs & Spices']\n  bars = ['Bars', 'Cocktail Bars', 'Sports Bars', 'Dive Bars', 'Speakeasies', 'Beer Gardens', 'Wine Bars', 'Tiki Bars', 'Whiskey Bars', 'Gay Bars', 'Irish Pub', 'Piano Bars', 'Jazz & Blues', 'Beer Bar', 'Pubs', 'Swimming Pools', 'Barbers', 'Lounges']\n  european = ['Italian', 'French', 'Greek', 'Turkish', 'Mediterranean', 'Moroccan', 'Spanish', 'Irish', 'British', 'German', 'Belgian', 'Scandinavian', 'Modern European', 'Ukrainian', 'Georgian', 'Brasseries']\n  asian = ['Chinese', 'Szechuan', 'Cantonese', 'Japanese', 'Korean', 'Thai', 'Vietnamese', 'Filipino', 'Malaysian', 'Himalayan/Nepalese', 'Dim Sum', 'Laotian', 'Asian Fusion', 'Pan Asian', 'Sushi Bars', 'Seafood', 'Ramen', 'Noodles']\n  middle_eastern = ['Indian', 'Afghan', 'Persian/Iranian', 'Lebanese', 'Egyptian', 'Ethiopian', 'Falafel']\n  latin = ['Mexican', 'Peruvian', 'Salvadoran', 'Venezuelan', 'Puerto Rican', 'Trinidadian', 'Brazilian', 'Latin American', 'Caribbean', 'Tacos', 'Tapas/Small Plates']\n  american = ['Public Markets', 'Pizza', 'New American', 'Burgers', 'Barbeque', 'American', 'Southern', 'Vegan', 'Steakhouses']\n  breakfast_lunch = ['Breakfast & Brunch', 'Specialty Food', 'Bagels', 'Salad', 'Sandwiches', 'Fish & Chips', 'Delis']\n  entertainment = ['Music Venues', 'Dance Clubs', 'Karaoke', 'Pool Halls', 'Performing Arts', 'Cultural Center', 'Golf', 'Mini Golf']\n  retail = ['Distilleries', 'Beer, Wine & Spirits', 'Breweries', 'Cheese Shops']\n  \n  if cuisine in cafes:\n    return 'cafes'\n  elif cuisine in bars:\n    return 'bars'\n  elif cuisine in european:\n    return 'european'\n  elif cuisine in asian:\n    return 'asian'\n  elif cuisine in middle_eastern:\n    return 'middle_eastern'\n  elif cuisine in latin:\n    return 'latin'\n  elif cuisine in american:\n    return 'american'\n  elif cuisine in breakfast_lunch:\n    return 'breakfast'\n  elif cuisine in entertainment:\n    return 'entertainment'\n  elif cuisine in retail:\n    return 'retail'\n  else:\n    return 'other'\n\nyelp_zip['cuisine_cat'] = yelp_zip['cuisine'].apply(categorize_cuisine)\n\n\n\nMissing Value Handling\nMissing values are dropped for rating and median income variables and were previously dropped for the price variable.\n\n# Finding missing values\nmissing_values = yelp_zip.isnull().sum()\nprint(missing_values)\n\n# count of missing values in price (zeros)\nprice_counts = yelp_zip['price'].value_counts()\nprint(price_counts)\n\n# count of missing values in income (zeros)\nprice_counts = yelp_zip['Median_Income'].value_counts()\nprint(price_counts)\n\n# count of missing values in zipcode (zeros)\nprice_counts = yelp_zip['zip_code'].value_counts()\nprint(price_counts)\n\n# count of missing values in review number (zeros)\nprice_counts = yelp_zip['review_count'].value_counts()\nprint(price_counts)\n\n# count of missing values in rating (zeros)\nprice_counts = yelp_zip['rating'].value_counts()\nprint(price_counts)\n\n# plot of missing rating values\n# Scatter plot of restaurant names and ratings\nplt.figure(figsize=(12, 6))\nsns.scatterplot(data=yelp_zip, x='name', y='rating', hue=(yelp_zip['rating'] == 0), palette={True: 'red', False: 'blue'})\nplt.xticks(rotation=90, fontsize=0)  # Rotate x-axis labels and set font size\nplt.title('Scatter plot of Missing Ratings by Restaurant')\nplt.xlabel('Restaurant Names')\nplt.ylabel('Ratings')\nplt.legend(title='Rating is Zero', loc='upper right', labels=['Non-zero', 'Zero'])\nplt.show()\n\n# dropping the rows that have a zero for 'rating'\nyelp_zip = yelp_zip[yelp_zip['rating'] != 0]\n# deleting the zero values for Median_Income\nyelp_zip = yelp_zip[yelp_zip['Median_Income'] != 0]\n\nprice_counts = yelp_zip['rating'].value_counts()\nprint(price_counts)\n\n# plot after fixing the missing ratings\nplt.figure(figsize=(12, 6))\nsns.scatterplot(data=yelp_zip, x='name', y='rating', hue=(yelp_zip['rating'] == 0), palette={True: 'red', False: 'blue'})\nplt.xticks(rotation=90, fontsize=0)  # Rotate x-axis labels and set font size\nplt.title('Scatter plot of Ratings by Restaurant After Handling Missing Values')\nplt.xlabel('Restaurant Names')\nplt.ylabel('Ratings')\nplt.legend(title='Rating is Zero', loc='upper right', labels=['Non-zero', 'Zero'])\nplt.show()\n\nMedian_Income    0\nzip_code         0\nname             0\ncuisine          0\nprice_range      0\nrating           0\nreview_count     0\nlatitude         0\nlongitude        0\nprice            0\ncuisine_cat      0\ndtype: int64\nprice\n20.5    387\n45.5     59\n5.0      52\n61.0      7\nName: count, dtype: int64\nMedian_Income\n133211    105\n107130     58\n132374     55\n0          47\n109147     45\n145048     27\n155054     26\n97694      25\n106930     25\n152955     24\n106560     15\n34352      13\n94820      12\n123134     10\n97507       7\n97327       6\n169489      3\n235511      1\n110375      1\nName: count, dtype: int64\nzip_code\n20001.0    105\n20002.0     58\n20009.0     55\n20005.0     45\n20007.0     27\n20003.0     26\n20024.0     25\n20036.0     25\n20004.0     24\n20010.0     15\n20006.0     13\n20037.0     12\n20008.0     10\n20017.0      7\n20011.0      6\n22201.0      6\n20910.0      4\n22202.0      4\n20016.0      3\n22203.0      3\n22314.0      3\n20000.0      2\n20735.0      2\n20052.0      2\n20560.0      2\n22209.0      2\n22003.0      2\n20814.0      2\n20781.0      2\n20740.0      2\n20012.0      1\n20045.0      1\n20015.0      1\n20737.0      1\n20057.0      1\n20852.0      1\n20902.0      1\n20912.0      1\n22204.0      1\n22205.0      1\n22304.0      1\nName: count, dtype: int64\nreview_count\n57.0     7\n74.0     5\n106.0    4\n25.0     4\n95.0     4\n        ..\n48.0     1\n255.0    1\n116.0    1\n105.0    1\n240.0    1\nName: count, Length: 351, dtype: int64\nrating\n4.3    68\n4.1    57\n4.4    55\n4.0    54\n4.2    47\n3.9    44\n3.8    36\n4.5    33\n4.6    24\n3.5    14\n3.3    11\n3.4    11\n3.7    11\n4.7     9\n3.6     9\n4.8     9\n2.9     5\n3.1     2\n5.0     2\n3.0     2\n4.9     1\n3.2     1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nrating\n4.3    61\n4.1    53\n4.0    53\n4.4    48\n4.2    46\n3.9    41\n3.8    36\n4.5    29\n4.6    16\n3.3    11\n3.5    11\n3.4    10\n3.7    10\n3.6     8\n4.7     7\n4.8     7\n2.9     5\n3.0     2\n3.1     1\n4.9     1\n5.0     1\n3.2     1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nOutlier Detection and Treatment\nIn this section, outliers are found for the numeric variables and removed for the review count variable, but retained for the other variables. This is because there were no overlapping outliers or clear issues with the outliers skewing the data for income and rating, thus, they were left in the dataset.\n\n# Outlier Detection using Z-Score\n\n# Median Income (Median_Income)\nyelp_zip['income_zscore'] = stats.zscore(yelp_zip['Median_Income'])\nzscore_outliers = yelp_zip[(yelp_zip['income_zscore'] &lt; -3) | (yelp_zip['income_zscore'] &gt; 3)]\nprint(\"Z-score income outliers:\", zscore_outliers)\n# yields one outlier: Open City at the National Cathedral in zip: 20015\n\n# # Rating (rating)\nyelp_zip['rating_zscore'] = stats.zscore(yelp_zip['rating'])\nzscore_outliers = yelp_zip[(yelp_zip['rating_zscore'] &lt; -3) | (yelp_zip['rating_zscore'] &gt; 3)]\nprint(\"Z-score rating outliers:\", zscore_outliers)\n# 6 rating outliers all with scores lower than 3.0\n\n# # Review Count (review_count)\nyelp_zip['review_zscore'] = stats.zscore(yelp_zip['review_count'])\nzscore_outliers = yelp_zip[(yelp_zip['review_zscore'] &lt; -3) | (yelp_zip['review_zscore'] &gt; 3)]\nprint(\"Z-score review outliers:\", zscore_outliers)\n# for our purposes the review_count outliers are removed after visualiatization:\n\n# Visualizing the Outliers with boxplots\n\n# Median Income Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['Median_Income'])\nplt.title('Boxplot of Median Income')\nplt.xlabel('Median Income')\nplt.show()\n\n# Ratings Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['rating'])\nplt.title('Boxplot of Yelp Ratings')\nplt.xlabel('Rating')\nplt.show()\n\n# Review count Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['review_count'])\nplt.title('Boxplot of Review Counts')\nplt.xlabel('Review Count')\nplt.show()\n\n# Price Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['price'])\nplt.title('Boxplot of Price')\nplt.xlabel('price')\nplt.show()\n\n# remove the review count outliers:\nyelp_zip = yelp_zip[(yelp_zip['review_zscore'] &gt;= -3) & (yelp_zip['review_zscore'] &lt;= 3)]\n\ncols_to_drop = ['rating_zscore', 'review_zscore', 'income_zscore']\nyelp_zip = yelp_zip.drop(columns=cols_to_drop)\n\n# Review count Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['review_count'])\nplt.title('Boxplot of Review Counts After Removing Outliers')\nplt.xlabel('Review Count')\nplt.show()\n\nZ-score income outliers:      Median_Income  zip_code                                 name  \\\n382          34352   20006.0      Filter Coffeehouse Foggy Bottom   \n383          34352   20006.0                Swing Coffee Roasters   \n384          34352   20006.0                       Compass Coffee   \n386          34352   20006.0                  Tatte Bakery & Cafe   \n388          34352   20006.0                    La Colombe Coffee   \n389          34352   20006.0                        Peet's Coffee   \n390          34352   20006.0                        54 Restaurant   \n392          34352   20006.0        Founding Farmers - Washington   \n394          34352   20006.0                       Off The Record   \n395          34352   20006.0    Immigrant Food at The White House   \n396          34352   20006.0                       Off The Record   \n398          34352   20006.0              Blackfinn Ameripub - DC   \n399          34352   20006.0                       Duke's Grocery   \n593         235511   20015.0  Open City At the National Cathedral   \n\n          cuisine  price_range  rating  review_count   latitude  longitude  \\\n382  Coffee & Tea            2     4.1         140.0  38.901188 -77.044282   \n383  Coffee & Tea            1     4.2         239.0  38.898096 -77.039904   \n384  Coffee & Tea            1     4.1         123.0  38.901043 -77.041429   \n386  Coffee & Tea            2     4.3          98.0  38.901248 -77.038866   \n388  Coffee & Tea            1     4.2         103.0  38.901190 -77.040157   \n389  Coffee & Tea            2     3.8         108.0  38.899230 -77.039980   \n390    Vietnamese            2     4.4         157.0  38.895764 -77.028640   \n392      American            2     4.0       17388.0  38.900366 -77.044435   \n394          Bars            3     4.1         409.0  38.900510 -77.037000   \n395  New American            2     4.2         232.0  38.899310 -77.040139   \n396          Bars            3     4.1         409.0  38.900510 -77.037000   \n398   Sports Bars            2     3.4         665.0  38.901060 -77.037500   \n399       Burgers            2     3.9         393.0  38.900516 -77.046014   \n593         Cafes            2     3.8         150.0  38.929811 -77.071355   \n\n     price cuisine_cat  income_zscore  \n382   20.5       cafes      -3.621439  \n383    5.0       cafes      -3.621439  \n384    5.0       cafes      -3.621439  \n386   20.5       cafes      -3.621439  \n388    5.0       cafes      -3.621439  \n389   20.5       cafes      -3.621439  \n390   20.5       asian      -3.621439  \n392   20.5    american      -3.621439  \n394   45.5        bars      -3.621439  \n395   20.5    american      -3.621439  \n396   45.5        bars      -3.621439  \n398   20.5        bars      -3.621439  \n399   20.5    american      -3.621439  \n593   20.5       cafes       4.734567  \nZ-score rating outliers:      Median_Income  zip_code                         name       cuisine  \\\n23          133211   20001.0                Corner Bakery      Bakeries   \n447         123134   20008.0                     Morsel's  Coffee & Tea   \n454         123134   20008.0                  Harry's Pub          Pubs   \n455         123134   20008.0                  Harry's Pub          Pubs   \n618          97694   20024.0  Casey's Coffee & Sandwiches  Coffee & Tea   \n636          97694   20024.0       Boardwalk Bar & Arcade          Bars   \n672         106930   20036.0                  The Rooftop       Lounges   \n\n     price_range  rating  review_count   latitude  longitude  price  \\\n23             2     2.9         205.0  38.896568 -77.009497   20.5   \n447            3     3.0           3.0  38.922855 -77.053824   45.5   \n454            2     2.9          57.0  38.924394 -77.054888   20.5   \n455            2     2.9          57.0  38.924394 -77.054888   20.5   \n618            2     2.9          25.0  38.883440 -77.016027   20.5   \n636            2     3.0         192.0  38.878480 -77.023990   20.5   \n672            3     2.9         104.0  38.910810 -77.045650   45.5   \n\n    cuisine_cat  income_zscore  rating_zscore  \n23        cafes       0.485095      -3.307778  \n447       cafes       0.066504      -3.030515  \n454        bars       0.066504      -3.307778  \n455        bars       0.066504      -3.307778  \n618       cafes      -0.990256      -3.307778  \n636        bars      -0.990256      -3.030515  \n672        bars      -0.606599      -3.307778  \nZ-score review outliers:      Median_Income  zip_code                               name       cuisine  \\\n45          133211   20001.0                           Zaytinya         Greek   \n341         109147   20005.0                   Old Ebbitt Grill          Bars   \n364         109147   20005.0                   Old Ebbitt Grill          Bars   \n392          34352   20006.0      Founding Farmers - Washington      American   \n408         145048   20007.0                      Baked & Wired      Bakeries   \n424         145048   20007.0  Founding Farmers Fishers & Bakers  New American   \n426         145048   20007.0                          il Canale       Italian   \n502         132374   20009.0                       Le Diplomate    Brasseries   \n\n     price_range  rating  review_count   latitude  longitude  price  \\\n45             3     4.2        6001.0  38.899040 -77.023490   45.5   \n341            2     4.2       11058.0  38.897967 -77.033342   20.5   \n364            2     4.2       11059.0  38.897967 -77.033342   20.5   \n392            2     4.0       17388.0  38.900366 -77.044435   20.5   \n408            2     4.4        5248.0  38.903913 -77.060248   20.5   \n424            2     3.8        4913.0  38.901784 -77.059733   20.5   \n426            2     4.3        4908.0  38.904493 -77.060974   20.5   \n502            3     4.3        5414.0  38.911350 -77.031570   45.5   \n\n    cuisine_cat  income_zscore  rating_zscore  review_zscore  \n45     european       0.485095       0.296635       4.104401  \n341        bars      -0.514506       0.296635       7.966897  \n364        bars      -0.514506       0.296635       7.967661  \n392    american      -3.621439      -0.257890      12.801700  \n408       cafes       0.976796       0.851159       3.529266  \n424    american       0.976796      -0.812415       3.273395  \n426    european       0.976796       0.573897       3.269576  \n502    european       0.450327       0.573897       3.656055  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization and Scaling\nIn this section we the distributions of the numerical variables, identifying skewness particularly. We will normalize the data in later analysis.\n\n# Normalization and Scaling\n\n# checking the skewness of the numeric variables\nskewness = yelp_zip[['Median_Income', 'price_range', 'rating', 'review_count', 'latitude', 'longitude', 'price']].skew()\nprint(skewness)\n\n# median income and review_count are highly skewed\n# plotting the skewed distributions\nplt.figure(figsize=(10, 6))\nsns.histplot(yelp_zip['Median_Income'], kde=True)\nplt.title('Distribution of Median Income')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n# negatively skewed\n\nplt.figure(figsize=(10, 6))\nsns.histplot(yelp_zip['review_count'], kde=True)\nplt.title('Distribution of Review Count')\nplt.xlabel('Review Count')\nplt.ylabel('Frequency')\nplt.show()\n# positively skewed\n\nMedian_Income   -0.829417\nprice_range      0.765710\nrating          -0.706162\nreview_count     2.780277\nlatitude         0.646110\nlongitude       -0.290954\nprice            1.412297\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# outputting the cleaned dataset\nyelp_zip.to_csv(\"clean_yelp_zip.csv\")\n\n\n\n\nSummary and Interpretation of Results\nWe have now dealt with the main and pressing issues of the dataset, so that it is ready for further analysis. The first step was reading in the sraped datasets and dealing with merging them into one complete dataset. The main issue resolved after merging was the issue of datatypes, particularly for the price variable. Initially, this variable was a string variable giving the price range score given by Yelp. We encoded this value based on the number of dollar-signs given as well as computed the average price in that range given by the Yelp price ranges on their website in the price_range variable. Both price and price_range are now categorical. The cuisine variable, which was categorical, was utilized to make a new variable, cuisine_cat, that groups the cuisine categories together to make it cleaner and easier to visualize in analysis (this was done with the aid of AI and through judgemnet decisions). We also dealt with missing values in the price and cuisine variables by removing these rows from the dataset. Additionally, we removed the z-score statistical outliers present in the review count variable. We finally checked for skewness, finding no immediately worrisome results and held off on normalization of the data. We are now ready to tackle Supervised and Unsupervised learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below:"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nLLM tools were used to help brainstorm data science questions to relate to our dataset and to understand what unsupervised and supervised tasks could be performed. Overall, LLM used to reformat and refocus our topic and project.\nAided in creating the groupings for the cuisine category manufactured variable."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nAiding in understanding and summarizing the literature review articles\nUnderstanding the interpretations of the t-SNE, PCA, and clustering plots"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\n\n\n\n\n\n\n\n\n\n\n\nInclude your data science questions on this page.\n\nWhat factors are most important to a high Yelp Rating score?\n\n\nDo better neighborhoods (measured by median income) generally have better restaurants (measured by Yelp ratings)?\n\n\nAre certain types of restaurants more likely to be rated higher on yelp?\n\n\nDoes price of the restaurant factor into Yelp rating?\n\n\nIs it possible to predict the Yelp rating of a restaurant based on its features?\n\n\n\n\n\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\n\n\n\n\n\n\nCharacterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews by Daniel Keller and Maria Kostromitina\n\nKeller, D., & Kostromitina, M. (2020). Characterizing non-chain restaurants’ Yelp star-ratings: Generalizable findings from a representative sample of Yelp reviews. International Journal of Hospitality Management, 86, 102440.\n\n\nIn “Characterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews” Keller and Kostromitina attempt to investigate whether Yelp star ratings are characterized by different criteria, ie is there a particular reason that restaurants are given the particular rating and is there a consistent pattern across these restaurants. To achieve this goal, the authors took data from Yelp.com that totaled 54,000 reviews on restaurants that were categorized as non-chain restaurants. The dataset includes the Yelp rating of the restaurant, the reviews, and other metadata about the specific restaurant. They then employed multiple correspondance analysis (MCA) to investigate the underlying structures and patterns of the data. They found that the service, food quality, and the overall environment of a restaurant does, in fact, matter for the Yelp rating of a particular restaurant. Further, they found that this effect was varied across the different star ratings for Yelp. For the 1 to 2 star range, the authors found that the most important factor contributing to the rating was the wait time for food. For the 3 star establishments, the most important factor was the quality of the food and for the 4 to 5 star rating range the most important factor was the ability to cultivate a positive customer experience, which is a combination of the wait time, the food quality, and the service.\n\nDoes Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp by Chun Kuang\n\nKuang, C. (2017). Does quality matter in local consumption amenities? An empirical investigation with Yelp. Journal of Urban Economics, 100, 1-18.\n\n\nKuang (2017) attempted to look at whether the quality of consumption anemities (ie restaurants) have a significant impact on the surronding local economy. In the paper “Does Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp”, the author utilized housing data from the D.C. Office of Tax and Revenue Computer Assisted Mass Appraisal Database coupled with restaurant data collected from Yelp.com to investigate the effect on housing prices of quality restaurants. The quality of a restaurant was measured by consumer ratings and price estimates from Yelp. To investigate this guiding question, Kuang employed econometric techniques including regression analysis, neighborhood-year fixed effects, and difference-in-difference (DID) estimation. The DID estimation was done before and after the popularization of Yelp in the DC area across time periods and restaurant measures. With the DID, the author wanted to determine whether information on restaurant amenitieis matters and particularly what type of information matters. In addition, robustness checks and falsification were employed to see if the results were robust to a restricted housing sample, a different choice of radius, including other local attributes, and utilizing a smaller timeframe. The end result was that highly rated restaurants do, in fact, attract customers, which generates revenues, and positively impacts the local economy. Further, that Yelp reviews do act as a reliable signal of quality for a restaurant. They conclude that there is a positive effect on housing prices due to both the quantity and quality of restaurants in a given area, but caution this do not imply causation and building quality anemities will not increase housing prices.\n\nYelp Review Rating Prediction: Machine Learning and Deep Learning Models by Zefang Liu\n\nLiu, Z. (2020). Yelp review rating prediction: Machine learning and deep learning models. arXiv preprint arXiv:2012.06690.\n\n\nIn “Yelp Review Rating Prediction: Machine Learning and Deep Learning Models”, Liu attempts to predict the yelp rating score based on the cumulative sentiment of the Yelp reviews for restaurants. The author begins by utilizing the Yelp Open dataset, which provides data on the businesses that are listed on the website. This dataset is subsetted to include only restaurants, which leads to 63,944 review observations. The author notes that the dataset is highly skewed due to the restaurants with more reviews, generally having a higher rating (in the 4-5 star yelp rating range). From these reviews, two vectorizer techniques are employed, TF-IDF and a count vectorizer. The author determines that the TF-IDf is found to be a better vectorizer for the review text based on evaluation metrics. From here, four machine learning models are trained and testing on the TF-IDF data: Naive Bayes, logistic regression, random forest, and linear support vector machine. In addition, four transformer models are trained and tested on the text data: BERT, DistillBERT, XLNet, and RoBERTa. Evaluation metrics including accuracy, F1-score, and confusion matrics are utilized to evaluate all of the models. The end result is that the model are able to partially reliably predict the Yelp rating score. Particularly, there was foudn to be a 64% accuracy score for prediction using the Machine Learning techniques and a 70% accuracy score for rating prediction using the transformer techniques.\n\nAnalysis of Yelp Review by Peter Hajas, Louis Gutierrez, and Mukkai S. Krishnamoorthy\n\nHajas, P., Gutierrez, L., & Krishnamoorthy, M. S. (2014). Analysis of yelp reviews. arXiv preprint arXiv:1407.1443.\n\n\nThis article examines the role of reviews and ratings in predicting business success, focusing specifically on restaurants in college towns. The authors highlight the unique characteristics of college-town restaurants, noting their relatively short average lifespan of around four years. The study analyzes data from 20 different college campuses over a 7-year period and employs mathematical modeling, including differential equations, to understand Yelp reviews. Key findings include the observation that the relationship between the number of reviews and ratings follows a power-law distribution. Additionally, restaurants with a higher number of reviews tend to cluster geographically. The researchers also analyze the running average rating of all restaurants in a college town over time, alongside detailed case studies of the most-reviewed restaurant in each town. The study reveals that initial reviews can significantly impact a restaurant’s trajectory, but these effects stabilize as more reviews accumulate. They attribute some trends to college students’ unique dining habits. For example, pizza restaurants consistently rank highly and are the most common across towns, while university-sponsored dining establishments often receive lower ratings. Interestingly, the types of top-rated ethnic restaurants vary between towns, reflecting local cultural preferences.\n\nPredicting the Helpfulness of Online Restaurant Reviews Using Different Machine Learning Algorithms: A Case Study of Yelp by Yi Luo and Xiaowei Xu\n\nLuo, Y., & Xu, X. (2019). Predicting the helpfulness of online restaurant reviews using different machine learning algorithms: A case study of yelp. Sustainability, 11(19), 5254.\n\n\nThis 2019 article delves into the importance of Yelp reviews and their role in the restaurant industry, highlighting the challenges restaurants face, such as rising food prices, high labor costs, and a failure rate exceeding 60% within the first three years. It underscores the economic significance of the restaurant industry, noting its contribution to local economic growth and employment rates. The article also emphasizes the critical role of online reviews, citing that 94% of people choose a restaurant based on reviews. The study focuses on two key aspects for machine learning models: dining features of restaurants and customer reviews. It employs models like Naive Bayes and Naive Bayes combined with Support Vector Machines (SVM), with SVM achieving the highest F1 accuracy at 71%. The research identifies four essential features influencing customer satisfaction: taste, experience, value, and location. It observes that with an increasing number of reviews, the quality of food emerges as the most important factor for customers, followed by service. Negative reviews are primarily linked to concerns about price or value. This work explores the intersection of natural language processing (NLP) and machine learning in analyzing Yelp reviews to predict business success, offering valuable insights into customer preferences and the factors driving restaurant performance\n\nApplications of Machine Learning to Predict Yelp Ratings by Kyle Carbon, Kacyn Jujii, and Parasanth Veerina\n\nCarbon, K., Fujii, K., & Veerina, P. (2014). Applications of machine learning to predict Yelp ratings. 2014.\n\n\nThe study “Applications of Machine Learning to Predict Yelp Ratings” by Kyle Carbon, Kacyn Jujii, and Parasanth Veerina investigates factors influencing Yelp ratings and business performance, utilizing data from Phoenix, AZ. The authors employ K-means clustering to evaluate the role of location by measuring distances to shopping malls and popular landmarks. They conduct statistical tests to identify significant features and implement machine learning models, including logistic regression, SVM, random forests, and decision trees, achieving an average accuracy of 45%. The findings highlight that while factors like location, price range, and availability of take-out services significantly impact ratings, the most critical determinant is review sentiment. Notably, different features were found to influence ratings for specific business types; for example, speed is prioritized for fast-food establishments, whereas quality is emphasized for upscale restaurants. Sentiment classification emerged as the most predictive feature, indicating that customer review sentiment is the strongest indicator of business ratings on Yelp.\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Landing page",
    "section": "",
    "text": "To begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Landing page",
    "section": "",
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\n\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInclude your data science questions on this page.\n\n\n\n\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\n\n\n\n\n\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Landing page",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website.\nThis report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods.\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\nThese are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights.\n\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nHI LIZZIE\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Tracking our progress and keeping a log of our contributions to the project."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with ChatGPT\nNarrow down topic and generate 5 guiding data science questions\nFind six related sources for literature review\nFind and scrape a relevant data source\nInvestigate the data and structure to ensure it fits our needs\nCreate a checklist to keep group on track\nMerge the data sources\nClean the combined dataset\nComplete EDA\nUnsupervised Learning tasks:\n\nPCA\nt-SNE\nK Means\nDBSCAN\nHierarchical\n\nSupervised Learning tasks:\n\nfeature selection\nnormalize data\nsplit into training/test sets\ndecision tree\nKNN regressor\nrandom forest\nlogistic regression\n\nReport:\n\nintro\nobjective\nkey findings\nmethodology\nvisualization\nsocietal impact\ncall to action\nconclusion\n\nIntrepret results for each section\nIntroduction and concluding remarks for each section\nAbout Me Page\nStylize and clean Landing Page\nFinalize LLM Usage Log\nDelete all the instructions\nClean up all sections of the webpage\nRender all portions\nMake sure Github is up to date\nHost on GU Domains\nTurn in all necessary submissions to Canvas"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nLizzie Healy About\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 11-13-2024\n\nMeeting with team to discuss potential topic ideas and brainstorm\n\nT: 11-19-2024\n\nMeeting to finalize topic, make deadlines, and distribute work\n\nF: 11-29-2024\n\nData cleaning and normalization\n\nM: 12-2-2024\n\nMeeting to look through data and re-group before next steps\n\nF: 12-6-2024\n\nComplete EDA\n\nW: 12-11-2024\n\nWork through unsupervised learning, add about me page\n\nT: 12-12-2024\n\nMeeting to discuss results of supervised/unsupervised learning\n\nF: 12-13-2024\n\nLiterature Review write-up, work on logs, finalize about me page\n\nS: 12-14-2024\n\nReport sections\n\nS: 12-15-2024\n\nMeeting to discuss any concerns/finalize website\n\nM: 12-16-2024\n\nFinal touches, render all sections, host on GU Domains"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nRachna Rawalpally About\nWeekly project contribution log:\nW: 11-13-2024\n\nMeeting with team to discuss potential topic ideas and brainstorm\n\nT: 11-19-2024\n\nMeeting to finalize topic, make deadlines, and distribute work\n\nF: 11-22-2024\n\nComplete data scraping and collection\n\nM: 12-2-2024\n\nMeeting to look through data and re-group before next steps\n\nW: 12-11-2024\n\nWork through supervised learning\n\nT: 12-12-2024\n\nMeeting to discuss results of supervised/unsupervised learning\n\nF: 12-13-2024\n\nData collection, supervised learning webpage writeups\n\nS: 12-14-2024\n\nReport sections\n\nS: 12-15-2024\n\nFinalize about me section\n\nS: 12-15-2024\n\nMeeting to discuss any concerns/finalize website\n\nM: 12-16-2024\n\nFinal touches, render all sections, host on GU Domains"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "API Key\nThe first step is to create and load our API key from Yelp. For security reasons, this key is stored elsewhere and is not directly included here. To create your own API, go to the Yelp’s developer portal to get started.\n\n#this imports the api to scrap\n# import json package \nimport json\n#opens and finds the api to load it \nwith open('/Users/rachnarawalpally/project-rachnarawalpally/technical-details/data-collection/api-key.json') as f:\n    keys = json.load(f)\n    #labels the api to call it in the code \nAPI_KEY = keys['yelp']\n\n\n\nMethod: Obtaining Data\nThe following code blocks outline the process of fetching data from the Yelp API and cleaning it into a readable CSV file.\nThe first block includes the necessary documentation to call the Yelp API and retrieve data. Some of this code is sourced directly from Yelp’s developer website.\nThe second block processes the received data by extracting only the relevant information needed for the CSV file. This includes gathering key details that are readily available on Yelp, such as restaurant name, rating, and location. The extracted data is then stored in a Pandas DataFrame.\nThe final block converts the Pandas DataFrame into a CSV file. Since each API response contains information for up to 50 restaurants, each CSV file includes data for a maximum of 50 Yelp rated restaurants.\n\n#reads in the request pacakge \nimport requests\n# this sets up the API request \nheaders = {'Authorization': f'Bearer {API_KEY}'}\n# this function fetches restaurant's data from Yelp's API\n# location = which area to search for \n# term = picks between restuarants, coffee shops, or bars (tried to get as much information about yelp reivews in DC)\n# limit and offset = these are specific for yelp, limit to search for 50 restaurants at a time\n# and start the offset at 0 so it goes to the next restaurant, like going to the next page \ndef fetch_restaurant_data(location, term=\"restaurants\", limit=50, offset=0):\n    url = 'https://api.yelp.com/v3/businesses/search'\n    params = {\n        'term': term,\n        'location': location,\n        'limit': limit,\n        'offset': offset,  # Include offset in the params\n    }\n    response = requests.get(url, headers=headers, params=params)\n    data = response.json()\n    return data\n\noffset = 0  # Start from 50 for the next set of results\nlimit = 50   # You want to get 50 results at a time\n\n# Fetch data for restaurants in Washington D.C.\nrestaurants = fetch_restaurant_data('Washington D.C.', offset=offset, limit=limit)\n\n\n# import package \nimport pandas as pd\n# this creates the function to clean up the json requests from above and clean it up and have it as a pandas dataframe \ndef process_data(restaurants_data):\n    # creates an emtpy list to store the data in \n    restaurant_list = []\n    for business in restaurants_data['businesses']:\n        # this adds all these specific items together \n        restaurant_list.append({\n            'name': business['name'],\n            'cuisine': business['categories'][0]['title'] if business['categories'] else 'Unknown',\n            'price_range': business.get('price', 'N/A'),\n            'rating': business.get('rating', 'N/A'),\n            'review_count': business.get('review_count','N/A'),\n            'neighborhoods': business.get('neighborhoods', 'N/A'),\n            'latitude': business['coordinates']['latitude'],\n            'longitude': business['coordinates']['longitude'],\n            'zip_code': business['location']['zip_code'],\n        })\n        #saves it as a dataframe \n    df = pd.DataFrame(restaurant_list)\n    return df\n# takes the function above to create a dataframe from the json information above \ndf_restaurants = process_data(restaurants)\n# prints the first few results to insure everything looks good \nprint(df_restaurants.head())\n\n                   name             cuisine price_range  rating  review_count  \\\n0  Unconventional Diner        New American          $$     4.4          2946   \n1             L'Ardente             Italian         $$$     4.5          1242   \n2          Grazie Nonna             Italian          $$     4.1           536   \n3      Old Ebbitt Grill                Bars          $$     4.2         11086   \n4      Gypsy Kitchen DC  Tapas/Small Plates          $$     4.3           919   \n\n  neighborhoods   latitude  longitude zip_code  \n0           N/A  38.906139 -77.023800    20001  \n1           N/A  38.898919 -77.014074    20001  \n2           N/A  38.904010 -77.035000    20005  \n3           N/A  38.897967 -77.033342    20005  \n4           N/A  38.914880 -77.031550    20009  \n\n\n\n# print the results again to insure everything is correct \n#print(df_restaurants)\n#saves the data frame to a csv file int he raw_data folder\ndf_restaurants.to_csv('../../data/raw-data/df_coffee5.csv')\n#/Users/rachnarawalpally/project-rachnarawalpally/data/raw-data\n\n                                   name                cuisine price_range  \\\n0               Pitango Gelato & Coffee           Coffee & Tea          $$   \n1                      Capital One Café           Coffee & Tea         N/A   \n2                    Mah-Ze-Dahr Bakery               Bakeries          $$   \n3        Junction Bistro Bar and Bakery               Bakeries          $$   \n4                          Coffee Alley           Coffee & Tea         N/A   \n5                       Gregorys Coffee               Bakeries          $$   \n6                           Atrium Cafe                  Cafes           $   \n7                         Mitsitam Cafe               American          $$   \n8                        Cafe Levantine               Lebanese         N/A   \n9                      Capital One Café  Banks & Credit Unions         N/A   \n10                               Zeleno             Sandwiches          $$   \n11                        Le Caprice DC                  Cafes           $   \n12                        Union Kitchen           Coffee & Tea          $$   \n13                       Bluestone Lane           Coffee & Tea         N/A   \n14              Corella Café and Lounge           Coffee & Tea          $$   \n15          Casey's Coffee & Sandwiches           Coffee & Tea          $$   \n16                Union Kitchen Grocery           Coffee & Tea          $$   \n17     L.A. Burdick Handmade Chocolates   Chocolatiers & Shops         N/A   \n18            Point Chaud Cafe & Crepes              Creperies          $$   \n19                     Vigilante Coffee           Coffee & Tea          $$   \n20                       Three Whistles   Shared Office Spaces           $   \n21           Adulis Coffee and Roastery           Coffee & Tea         N/A   \n22                          Timgad Café                  Cafes         N/A   \n23                         Café du Parc                 French          $$   \n24                             Morsel's           Coffee & Tea         $$$   \n25                           Sheba Café                  Cafes         N/A   \n26                           Licht Cafe                  Cafes         N/A   \n27                         Blank Street           Coffee & Tea         N/A   \n28                          Colada Shop           Coffee & Tea          $$   \n29                     Commonwealth Joe                  Cafes           $   \n30                          Uptown Cafe           Coffee & Tea           $   \n31                       Morning My Day               Bakeries           $   \n32                        Caseys Coffee           Coffee & Tea         N/A   \n33                    Merriweather Cafe                  Cafes         N/A   \n34                Soricha Tea & Theater           Coffee & Tea          $$   \n35                        Peet's Coffee           Coffee & Tea          $$   \n36                    La Bohemia Bakery               Bakeries           $   \n37                    Milk + Honey Café           Coffee & Tea         N/A   \n38                     Baker’s Daughter     Breakfast & Brunch         N/A   \n39                  Turkish Coffee Lady           Coffee & Tea          $$   \n40                         Cortado Cafe                  Cafes          $$   \n41  Tiger Sugar Boba Bubble Tea shop DC             Bubble Tea          $$   \n42   Call Your Mother Deli - Georgetown                  Delis          $$   \n43                        Cafe Integral           Coffee & Tea          $$   \n44                         Black Coffee           Coffee & Tea          $$   \n45                        Coffee Nature           Coffee & Tea           $   \n46                    Bread & Chocolate     Breakfast & Brunch          $$   \n47                          Le Bon Cafe           Coffee & Tea          $$   \n48                         Mo Mo Bakery               Bakeries           $   \n49                        The Hill Cafe           Coffee & Tea         N/A   \n\n    rating  review_count neighborhoods   latitude  longitude zip_code  \n0      4.2          1040           N/A  38.895058 -77.021854    20004  \n1      4.3            24           N/A  38.867232 -76.988468    20020  \n2      4.3           137           N/A  38.858644 -77.049471    22202  \n3      4.2            95           N/A  38.894935 -77.002259    20002  \n4      4.0             1           N/A  38.898571 -77.021774    20001  \n5      3.8            87           N/A  38.876988 -77.004496    20003  \n6      3.8           106           N/A  38.884277 -77.018194    20024  \n7      3.5           538           N/A  38.888184 -77.016863    20560  \n8      4.7            29           N/A  38.935452 -77.179605    22101  \n9      4.0            63           N/A  38.904992 -77.062633    20007  \n10     4.3            95           N/A  38.911483 -77.044128    20009  \n11     3.5           349           N/A  38.932815 -77.032744    20010  \n12     4.1            95           N/A  38.906762 -77.023699    20001  \n13     3.3            25           N/A  38.894308 -77.029739    20004  \n14     3.9            39           N/A  38.983660 -77.092950    20814  \n15     2.9            25           N/A  38.883440 -77.016027    20024  \n16     4.1            16           N/A  38.912090 -77.003690    20002  \n17     4.3            89           N/A  38.907180 -77.063050    20007  \n18     3.7            57           N/A  38.920154 -77.071873    20007  \n19     4.3           199           N/A  38.992035 -76.933845    20740  \n20     4.4           110           N/A  38.889560 -77.091200    22201  \n21     4.6             7           N/A  38.985367 -77.027355    20910  \n22     0.0             0           N/A  38.897080 -77.010790    20001  \n23     3.4           496           N/A  38.896491 -77.032656    20004  \n24     3.0             3           N/A  38.922855 -77.053824    20008  \n25     4.6             8           N/A  38.934610 -77.033200    20010  \n26     4.9            11           N/A  38.916837 -77.035461    20009  \n27     3.4            18           N/A  38.906160 -77.063010    20007  \n28     4.0            68           N/A  38.907064 -77.043662    20036  \n29     4.6           471           N/A  38.862669 -77.054934    22202  \n30     3.6            83           N/A  38.905458 -77.005096    20002  \n31     4.8            27           N/A  38.998113 -77.031311    20910  \n32     3.5            10           N/A  38.899840 -77.007540    20002  \n33     3.8            12           N/A  38.943640 -77.052620    20008  \n34     4.6           469           N/A  38.833030 -77.191437    22003  \n35     3.8           108           N/A  38.899230 -77.039980    20006  \n36     4.3           258           N/A  39.057995 -77.112355    20852  \n37     3.2             9           N/A  38.884751 -77.017456    20472  \n38     3.6            23           N/A  38.904533 -77.062452    20007  \n39     4.6           187           N/A  38.805617 -77.050411    22314  \n40     4.6           116           N/A  38.813170 -77.111088    22304  \n41     3.9            23           N/A  38.922077 -76.996569    20002  \n42     4.4           382           N/A  38.907617 -77.068837    20007  \n43     3.7            23           N/A  38.916040 -77.046870    20009  \n44     4.1           106           N/A  38.917900 -77.096820    20007  \n45     4.2           159           N/A  38.954480 -77.083120    20016  \n46     3.3           399           N/A  38.905660 -77.050450    20037  \n47     3.7           235           N/A  38.887260 -77.003357    20003  \n48     3.9            16           N/A  39.052131 -77.051026    20902  \n49     4.3            56           N/A  38.891074 -76.983391    20002  \n\n\n\n\nFinal Thoughts\nThis code demonstrates how to retrieve data from Yelp’s API to gather information on businesses such as restaurants, coffee shops, and bars (as used in this project). The script can be easily modified to query any other types of businesses available on Yelp. It provides a simple and efficient way to. Fetch Yelp data based on specific parameters. Convert the raw JSON response into a pandas DataFrame. Save the DataFrame as a CSV file for later analysis. By altering the search term (e.g., “restaurants”, “coffee shops”, or “bars”) and adjusting other parameters like limit and offset, you can customize the data retrieval to suit your needs. This method simplifies the process of gathering Yelp data for analysis and ensures easy access to relevant business information."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Step 1: Send a request to Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\nresponse = requests.get(url)\n\n# Step 2: Parse the page content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Step 3: Find the table containing the data (usually the first table for such lists)\ntable = soup.find('table', {'class': 'wikitable'})\n\n# Step 4: Extract data from the table rows\ncountries = []\npopulations = []\n\n# Iterate over the table rows\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    if len(cells) &gt; 1:\n        country = cells[1].text.strip()  # The country name is in the second column\n        population = cells[2].text.strip()  # The population is in the third column\n        countries.append(country)\n        populations.append(population)\n\n# Step 5: Create a DataFrame to store the results\ndata = pd.DataFrame({\n    'Country': countries,\n    'Population': populations\n})\n\n# Display the scraped data\nprint(data)\n\n# Optionally save to CSV\ndata.to_csv('../../data/raw-data/countries_population.csv', index=False)\n\n                                 Country     Population\n0                                  World  8,119,000,000\n1                                  China  1,409,670,000\n2                          1,404,910,000          17.3%\n3                          United States    335,893,238\n4                              Indonesia    282,477,584\n..                                   ...            ...\n235                   Niue (New Zealand)          1,681\n236                Tokelau (New Zealand)          1,647\n237                         Vatican City            764\n238  Cocos (Keeling) Islands (Australia)            593\n239                Pitcairn Islands (UK)             35\n\n[240 rows x 2 columns]"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts.\n\n\n#this imports the api to scrap \nimport json\nwith open('/Users/rachnarawalpally/project-rachnarawalpally/technical-details/data-collection/api-key.json') as f:\n    keys = json.load(f)\nAPI_KEY = keys['serpapi']\n\n\nimport pandas as pd \nstart_date = pd.Timestamp.today()\nend_date = start_date + pd.Timedelta(days=365)\ntravel_date = pd.date_range(start_date, end_date).strftime(\"%Y-%m-%d\").tolist()\nprint(travel_date)\n\n['2024-11-29', '2024-11-30', '2024-12-01', '2024-12-02', '2024-12-03', '2024-12-04', '2024-12-05', '2024-12-06', '2024-12-07', '2024-12-08', '2024-12-09', '2024-12-10', '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-14', '2024-12-15', '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20', '2024-12-21', '2024-12-22', '2024-12-23', '2024-12-24', '2024-12-25', '2024-12-26', '2024-12-27', '2024-12-28', '2024-12-29', '2024-12-30', '2024-12-31', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04', '2025-01-05', '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09', '2025-01-10', '2025-01-11', '2025-01-12', '2025-01-13', '2025-01-14', '2025-01-15', '2025-01-16', '2025-01-17', '2025-01-18', '2025-01-19', '2025-01-20', '2025-01-21', '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-25', '2025-01-26', '2025-01-27', '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31', '2025-02-01', '2025-02-02', '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06', '2025-02-07', '2025-02-08', '2025-02-09', '2025-02-10', '2025-02-11', '2025-02-12', '2025-02-13', '2025-02-14', '2025-02-15', '2025-02-16', '2025-02-17', '2025-02-18', '2025-02-19', '2025-02-20', '2025-02-21', '2025-02-22', '2025-02-23', '2025-02-24', '2025-02-25', '2025-02-26', '2025-02-27', '2025-02-28', '2025-03-01', '2025-03-02', '2025-03-03', '2025-03-04', '2025-03-05', '2025-03-06', '2025-03-07', '2025-03-08', '2025-03-09', '2025-03-10', '2025-03-11', '2025-03-12', '2025-03-13', '2025-03-14', '2025-03-15', '2025-03-16', '2025-03-17', '2025-03-18', '2025-03-19', '2025-03-20', '2025-03-21', '2025-03-22', '2025-03-23', '2025-03-24', '2025-03-25', '2025-03-26', '2025-03-27', '2025-03-28', '2025-03-29', '2025-03-30', '2025-03-31', '2025-04-01', '2025-04-02', '2025-04-03', '2025-04-04', '2025-04-05', '2025-04-06', '2025-04-07', '2025-04-08', '2025-04-09', '2025-04-10', '2025-04-11', '2025-04-12', '2025-04-13', '2025-04-14', '2025-04-15', '2025-04-16', '2025-04-17', '2025-04-18', '2025-04-19', '2025-04-20', '2025-04-21', '2025-04-22', '2025-04-23', '2025-04-24', '2025-04-25', '2025-04-26', '2025-04-27', '2025-04-28', '2025-04-29', '2025-04-30', '2025-05-01', '2025-05-02', '2025-05-03', '2025-05-04', '2025-05-05', '2025-05-06', '2025-05-07', '2025-05-08', '2025-05-09', '2025-05-10', '2025-05-11', '2025-05-12', '2025-05-13', '2025-05-14', '2025-05-15', '2025-05-16', '2025-05-17', '2025-05-18', '2025-05-19', '2025-05-20', '2025-05-21', '2025-05-22', '2025-05-23', '2025-05-24', '2025-05-25', '2025-05-26', '2025-05-27', '2025-05-28', '2025-05-29', '2025-05-30', '2025-05-31', '2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05', '2025-06-06', '2025-06-07', '2025-06-08', '2025-06-09', '2025-06-10', '2025-06-11', '2025-06-12', '2025-06-13', '2025-06-14', '2025-06-15', '2025-06-16', '2025-06-17', '2025-06-18', '2025-06-19', '2025-06-20', '2025-06-21', '2025-06-22', '2025-06-23', '2025-06-24', '2025-06-25', '2025-06-26', '2025-06-27', '2025-06-28', '2025-06-29', '2025-06-30', '2025-07-01', '2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05', '2025-07-06', '2025-07-07', '2025-07-08', '2025-07-09', '2025-07-10', '2025-07-11', '2025-07-12', '2025-07-13', '2025-07-14', '2025-07-15', '2025-07-16', '2025-07-17', '2025-07-18', '2025-07-19', '2025-07-20', '2025-07-21', '2025-07-22', '2025-07-23', '2025-07-24', '2025-07-25', '2025-07-26', '2025-07-27', '2025-07-28', '2025-07-29', '2025-07-30', '2025-07-31', '2025-08-01', '2025-08-02', '2025-08-03', '2025-08-04', '2025-08-05', '2025-08-06', '2025-08-07', '2025-08-08', '2025-08-09', '2025-08-10', '2025-08-11', '2025-08-12', '2025-08-13', '2025-08-14', '2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24', '2025-08-25', '2025-08-26', '2025-08-27', '2025-08-28', '2025-08-29', '2025-08-30', '2025-08-31', '2025-09-01', '2025-09-02', '2025-09-03', '2025-09-04', '2025-09-05', '2025-09-06', '2025-09-07', '2025-09-08', '2025-09-09', '2025-09-10', '2025-09-11', '2025-09-12', '2025-09-13', '2025-09-14', '2025-09-15', '2025-09-16', '2025-09-17', '2025-09-18', '2025-09-19', '2025-09-20', '2025-09-21', '2025-09-22', '2025-09-23', '2025-09-24', '2025-09-25', '2025-09-26', '2025-09-27', '2025-09-28', '2025-09-29', '2025-09-30', '2025-10-01', '2025-10-02', '2025-10-03', '2025-10-04', '2025-10-05', '2025-10-06', '2025-10-07', '2025-10-08', '2025-10-09', '2025-10-10', '2025-10-11', '2025-10-12', '2025-10-13', '2025-10-14', '2025-10-15', '2025-10-16', '2025-10-17', '2025-10-18', '2025-10-19', '2025-10-20', '2025-10-21', '2025-10-22', '2025-10-23', '2025-10-24', '2025-10-25', '2025-10-26', '2025-10-27', '2025-10-28', '2025-10-29', '2025-10-30', '2025-10-31', '2025-11-01', '2025-11-02', '2025-11-03', '2025-11-04', '2025-11-05', '2025-11-06', '2025-11-07', '2025-11-08', '2025-11-09', '2025-11-10', '2025-11-11', '2025-11-12', '2025-11-13', '2025-11-14', '2025-11-15', '2025-11-16', '2025-11-17', '2025-11-18', '2025-11-19', '2025-11-20', '2025-11-21', '2025-11-22', '2025-11-23', '2025-11-24', '2025-11-25', '2025-11-26', '2025-11-27', '2025-11-28', '2025-11-29']\n\n\n\nimport datetime\nimport json\nfrom serpapi import GoogleSearch\n\ndef search_google_flights(origin, destination, travel_date,departure_id, arrival_id, outbound_date, verbose=False, next_page_token=None):\n    # Set search parameters for Google Flights API \n    params = {\n        'api_key': API_KEY,                \n        'engine': 'google_flights',            \n        'q': f\"Flights from {origin} to {destination}\",  \n        'date': travel_date,  \n        'departure_id' : departure_id ,   \n        'arrival_id': arrival_id,\n        'outbound_date': outbound_date\n    }\n    filename = f\"flights_{origin}_to_{destination}.json\"\n    #result_dict = GoogleSearch(params).get_dict()\n    results = GoogleSearch(params).get_dict()\n    with open(filename, 'w') as f:\n            json.dump(results, f, indent=4)\n    print(f\"Best flights saved to {filename}\")\n\n\n\nbusy_airports  = [\n    \"MIA\", \"CLT\", \"LAS\",\"JFK\",\"ORD\",\"LAX\",\"DEN\",\"DFW\",\"ATL\",\"SFO\"\n]\norigin = \"IAD\"\n#travel_date = \"2024-12-12\"\ndeparture_id = \"IAD\"\narrival_id = [\n    \"MIA\", \"CLT\", \"LAS\",\"JFK\",\"ORD\",\"LAX\",\"DEN\",\"DFW\",\"ATL\",\"SFO\"\n]\noutbound_date = travel_date\n\n\nbusy_airports = [\"JFK\", \"LAX\", \"ORD\"]  # Example list of busy airports\norigin = \"SFO\"\ntravel_date = \"2024-12-25\"\n\n\nimport time \n\nfor destination in busy_airports:\n            print(f\"Searching flights from {origin} to {destination} on {travel_date}\")\n            next_page_token = search_google_flights(origin, destination, travel_date,departure_id,arrival_id,outbound_date,  verbose=True)\n            time.sleep(2)\n\nSearching flights from IAD to MIA on ['2024-11-29', '2024-11-30', '2024-12-01', '2024-12-02', '2024-12-03', '2024-12-04', '2024-12-05', '2024-12-06', '2024-12-07', '2024-12-08', '2024-12-09', '2024-12-10', '2024-12-11', '2024-12-12', '2024-12-13', '2024-12-14', '2024-12-15', '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20', '2024-12-21', '2024-12-22', '2024-12-23', '2024-12-24', '2024-12-25', '2024-12-26', '2024-12-27', '2024-12-28', '2024-12-29', '2024-12-30', '2024-12-31', '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04', '2025-01-05', '2025-01-06', '2025-01-07', '2025-01-08', '2025-01-09', '2025-01-10', '2025-01-11', '2025-01-12', '2025-01-13', '2025-01-14', '2025-01-15', '2025-01-16', '2025-01-17', '2025-01-18', '2025-01-19', '2025-01-20', '2025-01-21', '2025-01-22', '2025-01-23', '2025-01-24', '2025-01-25', '2025-01-26', '2025-01-27', '2025-01-28', '2025-01-29', '2025-01-30', '2025-01-31', '2025-02-01', '2025-02-02', '2025-02-03', '2025-02-04', '2025-02-05', '2025-02-06', '2025-02-07', '2025-02-08', '2025-02-09', '2025-02-10', '2025-02-11', '2025-02-12', '2025-02-13', '2025-02-14', '2025-02-15', '2025-02-16', '2025-02-17', '2025-02-18', '2025-02-19', '2025-02-20', '2025-02-21', '2025-02-22', '2025-02-23', '2025-02-24', '2025-02-25', '2025-02-26', '2025-02-27', '2025-02-28', '2025-03-01', '2025-03-02', '2025-03-03', '2025-03-04', '2025-03-05', '2025-03-06', '2025-03-07', '2025-03-08', '2025-03-09', '2025-03-10', '2025-03-11', '2025-03-12', '2025-03-13', '2025-03-14', '2025-03-15', '2025-03-16', '2025-03-17', '2025-03-18', '2025-03-19', '2025-03-20', '2025-03-21', '2025-03-22', '2025-03-23', '2025-03-24', '2025-03-25', '2025-03-26', '2025-03-27', '2025-03-28', '2025-03-29', '2025-03-30', '2025-03-31', '2025-04-01', '2025-04-02', '2025-04-03', '2025-04-04', '2025-04-05', '2025-04-06', '2025-04-07', '2025-04-08', '2025-04-09', '2025-04-10', '2025-04-11', '2025-04-12', '2025-04-13', '2025-04-14', '2025-04-15', '2025-04-16', '2025-04-17', '2025-04-18', '2025-04-19', '2025-04-20', '2025-04-21', '2025-04-22', '2025-04-23', '2025-04-24', '2025-04-25', '2025-04-26', '2025-04-27', '2025-04-28', '2025-04-29', '2025-04-30', '2025-05-01', '2025-05-02', '2025-05-03', '2025-05-04', '2025-05-05', '2025-05-06', '2025-05-07', '2025-05-08', '2025-05-09', '2025-05-10', '2025-05-11', '2025-05-12', '2025-05-13', '2025-05-14', '2025-05-15', '2025-05-16', '2025-05-17', '2025-05-18', '2025-05-19', '2025-05-20', '2025-05-21', '2025-05-22', '2025-05-23', '2025-05-24', '2025-05-25', '2025-05-26', '2025-05-27', '2025-05-28', '2025-05-29', '2025-05-30', '2025-05-31', '2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05', '2025-06-06', '2025-06-07', '2025-06-08', '2025-06-09', '2025-06-10', '2025-06-11', '2025-06-12', '2025-06-13', '2025-06-14', '2025-06-15', '2025-06-16', '2025-06-17', '2025-06-18', '2025-06-19', '2025-06-20', '2025-06-21', '2025-06-22', '2025-06-23', '2025-06-24', '2025-06-25', '2025-06-26', '2025-06-27', '2025-06-28', '2025-06-29', '2025-06-30', '2025-07-01', '2025-07-02', '2025-07-03', '2025-07-04', '2025-07-05', '2025-07-06', '2025-07-07', '2025-07-08', '2025-07-09', '2025-07-10', '2025-07-11', '2025-07-12', '2025-07-13', '2025-07-14', '2025-07-15', '2025-07-16', '2025-07-17', '2025-07-18', '2025-07-19', '2025-07-20', '2025-07-21', '2025-07-22', '2025-07-23', '2025-07-24', '2025-07-25', '2025-07-26', '2025-07-27', '2025-07-28', '2025-07-29', '2025-07-30', '2025-07-31', '2025-08-01', '2025-08-02', '2025-08-03', '2025-08-04', '2025-08-05', '2025-08-06', '2025-08-07', '2025-08-08', '2025-08-09', '2025-08-10', '2025-08-11', '2025-08-12', '2025-08-13', '2025-08-14', '2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24', '2025-08-25', '2025-08-26', '2025-08-27', '2025-08-28', '2025-08-29', '2025-08-30', '2025-08-31', '2025-09-01', '2025-09-02', '2025-09-03', '2025-09-04', '2025-09-05', '2025-09-06', '2025-09-07', '2025-09-08', '2025-09-09', '2025-09-10', '2025-09-11', '2025-09-12', '2025-09-13', '2025-09-14', '2025-09-15', '2025-09-16', '2025-09-17', '2025-09-18', '2025-09-19', '2025-09-20', '2025-09-21', '2025-09-22', '2025-09-23', '2025-09-24', '2025-09-25', '2025-09-26', '2025-09-27', '2025-09-28', '2025-09-29', '2025-09-30', '2025-10-01', '2025-10-02', '2025-10-03', '2025-10-04', '2025-10-05', '2025-10-06', '2025-10-07', '2025-10-08', '2025-10-09', '2025-10-10', '2025-10-11', '2025-10-12', '2025-10-13', '2025-10-14', '2025-10-15', '2025-10-16', '2025-10-17', '2025-10-18', '2025-10-19', '2025-10-20', '2025-10-21', '2025-10-22', '2025-10-23', '2025-10-24', '2025-10-25', '2025-10-26', '2025-10-27', '2025-10-28', '2025-10-29', '2025-10-30', '2025-10-31', '2025-11-01', '2025-11-02', '2025-11-03', '2025-11-04', '2025-11-05', '2025-11-06', '2025-11-07', '2025-11-08', '2025-11-09', '2025-11-10', '2025-11-11', '2025-11-12', '2025-11-13', '2025-11-14', '2025-11-15', '2025-11-16', '2025-11-17', '2025-11-18', '2025-11-19', '2025-11-20', '2025-11-21', '2025-11-22', '2025-11-23', '2025-11-24', '2025-11-25', '2025-11-26', '2025-11-27', '2025-11-28', '2025-11-29']\n\n\n\n---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\nCell In[113], line 5\n      3 for destination in busy_airports:\n      4             print(f\"Searching flights from {origin} to {destination} on {travel_date}\")\n----&gt; 5             next_page_token = search_google_flights(origin, destination, travel_date,departure_id,arrival_id,outbound_date,  verbose=True)\n      6             time.sleep(2)\n\nCell In[111], line 18, in search_google_flights(origin, destination, travel_date, departure_id, arrival_id, outbound_date, verbose, next_page_token)\n     16 filename = f\"flights_{origin}_to_{destination}.json\"\n     17 #result_dict = GoogleSearch(params).get_dict()\n---&gt; 18 results = GoogleSearch(params).get_dict()\n     19 with open(filename, 'w') as f:\n     20         json.dump(results, f, indent=4)\n\nFile /opt/anaconda3/lib/python3.11/site-packages/serpapi/serp_api_client.py:103, in SerpApiClient.get_dict(self)\n     98 def get_dict(self):\n     99     \"\"\"Returns:\n    100         Dict with the formatted response content\n    101         (alias for get_dictionary)\n    102     \"\"\"\n--&gt; 103     return self.get_dictionary()\n\nFile /opt/anaconda3/lib/python3.11/site-packages/serpapi/serp_api_client.py:96, in SerpApiClient.get_dictionary(self)\n     92 def get_dictionary(self):\n     93     \"\"\"Returns:\n     94         Dict with the formatted response content\n     95     \"\"\"\n---&gt; 96     return dict(self.get_json())\n\nFile /opt/anaconda3/lib/python3.11/site-packages/serpapi/serp_api_client.py:83, in SerpApiClient.get_json(self)\n     79 \"\"\"Returns:\n     80     Formatted JSON search results using json package\n     81 \"\"\"\n     82 self.params_dict[\"output\"] = \"json\"\n---&gt; 83 return json.loads(self.get_results())\n\nFile /opt/anaconda3/lib/python3.11/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    341     s = s.decode(detect_encoding(s), 'surrogatepass')\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 346     return _default_decoder.decode(s)\n    347 if cls is None:\n    348     cls = JSONDecoder\n\nFile /opt/anaconda3/lib/python3.11/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    332 def decode(self, s, _w=WHITESPACE.match):\n    333     \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334     containing a JSON document).\n    335 \n    336     \"\"\"\n--&gt; 337     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338     end = _w(s, end).end()\n    339     if end != len(s):\n\nFile /opt/anaconda3/lib/python3.11/json/decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\n    353     obj, end = self.scan_once(s, idx)\n    354 except StopIteration as err:\n--&gt; 355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    356 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n\n\n\n#this imports the api to scrap \nimport json\nwith open('/Users/rachnarawalpally/project-rachnarawalpally/technical-details/data-collection/api-key.json') as f:\n    keys = json.load(f)\nAPI_KEY = keys['yelp']\n\n\n\nheaders = {'Authorization': f'Bearer {API_KEY}'}\n\ndef fetch_restaurant_data(location, term=\"coffee shops\", limit=50, offset=0):\n    url = 'https://api.yelp.com/v3/businesses/search'\n    params = {\n        'term': term,\n        'location': location,\n        'limit': limit,\n        'offset': offset,  # Include offset in the params\n    }\n    response = requests.get(url, headers=headers, params=params)\n    data = response.json()\n    return data\n\noffset = 190  # Start from 50 for the next set of results\nlimit = 50   # You want to fetch 50 results at a time\n\n# Fetch data for restaurants in Washington D.C.\nrestaurants = fetch_restaurant_data('Washington D.C.', offset=offset, limit=limit)\n\n\nimport pandas as pd\n\ndef process_data(restaurants_data):\n    restaurant_list = []\n    for business in restaurants_data['businesses']:\n        restaurant_list.append({\n            'name': business['name'],\n            'cuisine': business['categories'][0]['title'] if business['categories'] else 'Unknown',\n            'price_range': business.get('price', 'N/A'),\n            'rating': business.get('rating', 'N/A'),\n            'review_count': business.get('review_count','N/A'),\n            'neighborhoods': business.get('neighborhoods', 'N/A'),\n            'latitude': business['coordinates']['latitude'],\n            'longitude': business['coordinates']['longitude'],\n            'zip_code': business['location']['zip_code'],\n        })\n    df = pd.DataFrame(restaurant_list)\n    return df\ndf_restaurants = process_data(restaurants)\n\n\nprint(df_restaurants)\ndf_restaurants.to_csv('../../data/raw-data/df_coffee5.csv')\n#/Users/rachnarawalpally/project-rachnarawalpally/data/raw-data\n\n                                   name                cuisine price_range  \\\n0               Pitango Gelato & Coffee           Coffee & Tea          $$   \n1                      Capital One Café           Coffee & Tea         N/A   \n2                    Mah-Ze-Dahr Bakery               Bakeries          $$   \n3        Junction Bistro Bar and Bakery               Bakeries          $$   \n4                          Coffee Alley           Coffee & Tea         N/A   \n5                       Gregorys Coffee               Bakeries          $$   \n6                           Atrium Cafe                  Cafes           $   \n7                         Mitsitam Cafe               American          $$   \n8                        Cafe Levantine               Lebanese         N/A   \n9                      Capital One Café  Banks & Credit Unions         N/A   \n10                               Zeleno             Sandwiches          $$   \n11                        Le Caprice DC                  Cafes           $   \n12                        Union Kitchen           Coffee & Tea          $$   \n13                       Bluestone Lane           Coffee & Tea         N/A   \n14              Corella Café and Lounge           Coffee & Tea          $$   \n15          Casey's Coffee & Sandwiches           Coffee & Tea          $$   \n16                Union Kitchen Grocery           Coffee & Tea          $$   \n17     L.A. Burdick Handmade Chocolates   Chocolatiers & Shops         N/A   \n18            Point Chaud Cafe & Crepes              Creperies          $$   \n19                     Vigilante Coffee           Coffee & Tea          $$   \n20                       Three Whistles   Shared Office Spaces           $   \n21           Adulis Coffee and Roastery           Coffee & Tea         N/A   \n22                          Timgad Café                  Cafes         N/A   \n23                         Café du Parc                 French          $$   \n24                             Morsel's           Coffee & Tea         $$$   \n25                           Sheba Café                  Cafes         N/A   \n26                           Licht Cafe                  Cafes         N/A   \n27                         Blank Street           Coffee & Tea         N/A   \n28                          Colada Shop           Coffee & Tea          $$   \n29                     Commonwealth Joe                  Cafes           $   \n30                          Uptown Cafe           Coffee & Tea           $   \n31                       Morning My Day               Bakeries           $   \n32                        Caseys Coffee           Coffee & Tea         N/A   \n33                    Merriweather Cafe                  Cafes         N/A   \n34                Soricha Tea & Theater           Coffee & Tea          $$   \n35                        Peet's Coffee           Coffee & Tea          $$   \n36                    La Bohemia Bakery               Bakeries           $   \n37                    Milk + Honey Café           Coffee & Tea         N/A   \n38                     Baker’s Daughter     Breakfast & Brunch         N/A   \n39                  Turkish Coffee Lady           Coffee & Tea          $$   \n40                         Cortado Cafe                  Cafes          $$   \n41  Tiger Sugar Boba Bubble Tea shop DC             Bubble Tea          $$   \n42   Call Your Mother Deli - Georgetown                  Delis          $$   \n43                        Cafe Integral           Coffee & Tea          $$   \n44                         Black Coffee           Coffee & Tea          $$   \n45                        Coffee Nature           Coffee & Tea           $   \n46                    Bread & Chocolate     Breakfast & Brunch          $$   \n47                          Le Bon Cafe           Coffee & Tea          $$   \n48                         Mo Mo Bakery               Bakeries           $   \n49                        The Hill Cafe           Coffee & Tea         N/A   \n\n    rating  review_count neighborhoods   latitude  longitude zip_code  \n0      4.2          1040           N/A  38.895058 -77.021854    20004  \n1      4.3            24           N/A  38.867232 -76.988468    20020  \n2      4.3           137           N/A  38.858644 -77.049471    22202  \n3      4.2            95           N/A  38.894935 -77.002259    20002  \n4      4.0             1           N/A  38.898571 -77.021774    20001  \n5      3.8            87           N/A  38.876988 -77.004496    20003  \n6      3.8           106           N/A  38.884277 -77.018194    20024  \n7      3.5           538           N/A  38.888184 -77.016863    20560  \n8      4.7            29           N/A  38.935452 -77.179605    22101  \n9      4.0            63           N/A  38.904992 -77.062633    20007  \n10     4.3            95           N/A  38.911483 -77.044128    20009  \n11     3.5           349           N/A  38.932815 -77.032744    20010  \n12     4.1            95           N/A  38.906762 -77.023699    20001  \n13     3.3            25           N/A  38.894308 -77.029739    20004  \n14     3.9            39           N/A  38.983660 -77.092950    20814  \n15     2.9            25           N/A  38.883440 -77.016027    20024  \n16     4.1            16           N/A  38.912090 -77.003690    20002  \n17     4.3            89           N/A  38.907180 -77.063050    20007  \n18     3.7            57           N/A  38.920154 -77.071873    20007  \n19     4.3           199           N/A  38.992035 -76.933845    20740  \n20     4.4           110           N/A  38.889560 -77.091200    22201  \n21     4.6             7           N/A  38.985367 -77.027355    20910  \n22     0.0             0           N/A  38.897080 -77.010790    20001  \n23     3.4           496           N/A  38.896491 -77.032656    20004  \n24     3.0             3           N/A  38.922855 -77.053824    20008  \n25     4.6             8           N/A  38.934610 -77.033200    20010  \n26     4.9            11           N/A  38.916837 -77.035461    20009  \n27     3.4            18           N/A  38.906160 -77.063010    20007  \n28     4.0            68           N/A  38.907064 -77.043662    20036  \n29     4.6           471           N/A  38.862669 -77.054934    22202  \n30     3.6            83           N/A  38.905458 -77.005096    20002  \n31     4.8            27           N/A  38.998113 -77.031311    20910  \n32     3.5            10           N/A  38.899840 -77.007540    20002  \n33     3.8            12           N/A  38.943640 -77.052620    20008  \n34     4.6           469           N/A  38.833030 -77.191437    22003  \n35     3.8           108           N/A  38.899230 -77.039980    20006  \n36     4.3           258           N/A  39.057995 -77.112355    20852  \n37     3.2             9           N/A  38.884751 -77.017456    20472  \n38     3.6            23           N/A  38.904533 -77.062452    20007  \n39     4.6           187           N/A  38.805617 -77.050411    22314  \n40     4.6           116           N/A  38.813170 -77.111088    22304  \n41     3.9            23           N/A  38.922077 -76.996569    20002  \n42     4.4           382           N/A  38.907617 -77.068837    20007  \n43     3.7            23           N/A  38.916040 -77.046870    20009  \n44     4.1           106           N/A  38.917900 -77.096820    20007  \n45     4.2           159           N/A  38.954480 -77.083120    20016  \n46     3.3           399           N/A  38.905660 -77.050450    20037  \n47     3.7           235           N/A  38.887260 -77.003357    20003  \n48     3.9            16           N/A  39.052131 -77.051026    20902  \n49     4.3            56           N/A  38.891074 -76.983391    20002"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/eda/explore_data.html",
    "href": "technical-details/eda/explore_data.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import f_oneway\n\n\n# reading in the data\nyelp_zip = pd.read_csv(\"../data-cleaning/clean_yelp_zip.csv\")\nprint(yelp_zip.dtypes)\n\ncols_to_drop = \"Unnamed: 0\"\nyelp_zip = yelp_zip.drop(columns=cols_to_drop)\nprint(yelp_zip.head(5))\n\nUnnamed: 0         int64\nMedian_Income      int64\nzip_code           int64\nname              object\ncuisine           object\nprice_range        int64\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\nprice            float64\ndtype: object\n   Median_Income  zip_code                              name       cuisine  \\\n0         133211     20001                    Compass Coffee  Coffee & Tea   \n1         133211     20001  Marianne’s by DC Central Kitchen         Cafes   \n2         133211     20001        Sankofa Video Books & Cafe    Bookstores   \n3         133211     20001                 La Colombe Coffee  Coffee & Tea   \n4         133211     20001              Sylvan Cafe & Bakery      Bakeries   \n\n   price_range  rating  review_count   latitude  longitude  price  \n0            2     4.1          92.0  38.916256 -77.022773   20.5  \n1            0     4.6          17.0  38.898720 -77.024770    0.0  \n2            1     4.5         167.0  38.925561 -77.023150    5.0  \n3            2     4.0         303.0  38.901051 -77.020103   20.5  \n4            1     4.0         134.0  38.915393 -77.012592    5.0  \n\n\n\n# numerical variables\n\n# pie chart for zipcode\nzip_counts = yelp_zip['zip_code'].value_counts()\ntop_15 = zip_counts.head(15)\ntop_15.plot(kind='pie', figsize=(8, 8), autopct='%1.1f%%')\nplt.title('Top 15 Zipcodes with Most Restaurants')\nplt.ylabel('')\nplt.show()\n\n# distribution of median_income\nyelp_zip['Median_Income'].plot(kind='kde', figsize=(10, 6))\nplt.title('Density Plot of ZipCode Median Income')\nplt.xlabel('Median Income')\nplt.ylabel('Density')\nplt.show()\n\n# distribution of rating\nyelp_zip['rating'].plot(kind='hist', bins=30, figsize=(10, 6))\nplt.title('Distribution of Ratings')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# categorical variables (name and cuisine)\n# bar chart of cuisine\ncuisine_counts = yelp_zip['cuisine'].value_counts()\ntop_15 = cuisine_counts.head(15)\ntop_15.plot(kind='bar', figsize=(10, 6))\nplt.title('Counts for 15 Most Popular Cuisine Categories')\nplt.xlabel('Cuisine')\nplt.ylabel('Count')\nplt.show()\n\n# pie chart\ntop_10 = cuisine_counts.head(10)\ntop_10.plot(kind='pie', figsize=(8, 8), autopct='%1.1f%%')\nplt.title('Percentage of 10 Most Popular Cuisine Categories')\nplt.ylabel('')\nplt.show()\n\n# pie chart of price_range\nprice_range = yelp_zip['price_range'].value_counts()\nprice_range.plot(kind='pie', figsize=(8, 8), autopct='%1.1f%%')\nplt.title('Percentage of Restaurant by Yelp Price Ranking')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Correlation Analysis\n\n# median_income and rating\ncorrelation_matrix = yelp_zip[['Median_Income', 'rating']].corr()\nprint(correlation_matrix)\n\n# median_income and price_range\ncorrelation_matrix = yelp_zip[['Median_Income', 'price_range']].corr()\nprint(correlation_matrix)\n\n# price_range and rating\ncorrelation_matrix = yelp_zip[['price_range', 'rating']].corr()\nprint(correlation_matrix)\n\n# Heat Map of Numeric\nnumeric_only = yelp_zip.filter(items=['Median_Income', 'rating', 'price_range', 'review_count', 'price', 'zip_code'])\ncorr = numeric_only.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Heatmap of Selected Genre Correlations')\nplt.show()\n\n               Median_Income   rating\nMedian_Income        1.00000 -0.10298\nrating              -0.10298  1.00000\n               Median_Income  price_range\nMedian_Income       1.000000     0.032961\nprice_range         0.032961     1.000000\n             price_range    rating\nprice_range     1.000000 -0.206732\nrating         -0.206732  1.000000\n\n\n\n\n\n\n\n\n\n\n# Crosstabulations\n\n# cuisine, price_range\ntop_20 = cuisine_counts.head(20).index\ncuisine_price_crosstab = pd.crosstab(yelp_zip[yelp_zip['cuisine'].isin(top_20)]['cuisine'], yelp_zip['price_range'])\nprint(cuisine_price_crosstab)\ncuisine_price_crosstab.plot(kind='bar', stacked=True, figsize=(14, 8))\nplt.title('Crosstabulation of Top 20 Cuisine Types and Price Range')\nplt.xlabel('Cuisine Type (top 20)')\nplt.ylabel('Count')\nplt.legend(title='Price Range')\nplt.show()\n\nprice_range          0   1   2   3  4\ncuisine                              \nAmerican             1   0  11   0  0\nBakeries             7   6  18   0  0\nBars                19   2  30   8  0\nBreakfast & Brunch   3   0  10   0  0\nBreweries            2   1   6   0  0\nCafes               11   5   7   2  0\nChinese              0   0   8   0  0\nCocktail Bars       14   0  22  11  0\nCoffee & Tea        54  23  78   1  0\nDive Bars            6   1   3   0  0\nFrench               4   0   7   2  0\nItalian              5   0  10   5  0\nLounges              3   0   4   2  1\nMexican              1   1   6   3  0\nNew American         7   0  27   8  3\nPubs                 2   0   7   0  0\nRamen                0   0   7   1  0\nSeafood              3   0   3   1  0\nSports Bars          4   1  10   0  0\nWine Bars            8   0   5   2  0\n\n\n\n\n\n\n\n\n\n\n# Feature Pairings\n# rating is the target, with Median_Income or Zipcode as the feature\n\n# scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(yelp_zip['rating'], yelp_zip['Median_Income'], alpha=0.5)\nplt.title('Scatter Plot of Median Income vs Rating')\nplt.xlabel('Median Income')\nplt.ylabel('Rating')\nplt.show()\n\n# scatter plot by price_range\nplt.figure(figsize=(10, 6))\nfor price in yelp_zip['price_range'].unique():\n    subset = yelp_zip[yelp_zip['price_range'] == price]\n    plt.scatter(subset['rating'], subset['Median_Income'], alpha=0.5, label=f'Price Range {price}')\nplt.title('Scatter Plot of Rating vs Median Income by Price Range')\nplt.xlabel('Median Income')\nplt.ylabel('Rating')\nplt.legend()\nplt.show()\n\n# number of ratings\nplt.figure(figsize=(10, 6))\nplt.scatter(yelp_zip['rating'], yelp_zip['review_count'], alpha=0.5)\nplt.title('Scatter Plot of Rating vs Review Count')\nplt.xlabel('Review Count')\nplt.ylabel('Rating')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# t-tests, chi-square, ANOVA\n\n# pearson correlation\npearson_corr = yelp_zip[['rating', 'Median_Income']].corr(method='pearson')\nprint(pearson_corr)\n\n# chi-square for ratings and cuisine type\ncontingency_table = pd.crosstab(yelp_zip['rating'], yelp_zip['cuisine'])\nchi2, p, dof, expected = chi2_contingency(contingency_table)\nprint(f\"Chi-square statistic: {chi2}\")\nprint(f\"P-value: {p}\")\nprint(f\"Degrees of freedom: {dof}\")\n\n# chi-square for cuisine type and price range\ncontingency_table_cuisine_price = pd.crosstab(yelp_zip['cuisine'], yelp_zip['price_range'])\nchi2_cuisine_price, p_cuisine_price, dof_cuisine_price, expected_cuisine_price = chi2_contingency(contingency_table_cuisine_price)\nprint(f\"Chi-square statistic for cuisine and price range: {chi2_cuisine_price}\")\nprint(f\"P-value for cuisine and price range: {p_cuisine_price}\")\nprint(f\"Degrees of freedom for cuisine and price range: {dof_cuisine_price}\")\n\n# ANOVA for ratings and cuisine type\nanova_result = f_oneway(*(yelp_zip[yelp_zip['cuisine'] == cuisine]['rating'] for cuisine in yelp_zip['cuisine'].unique()))\nprint(f\"ANOVA F-statistic: {anova_result.statistic}\")\nprint(f\"P-value: {anova_result.pvalue}\")\n\n                rating  Median_Income\nrating         1.00000       -0.10298\nMedian_Income -0.10298        1.00000\nChi-square statistic: 2595.760822560985\nP-value: 0.17724122058522332\nDegrees of freedom: 2530\nChi-square statistic for cuisine and price range: 638.6815434836537\nP-value for cuisine and price range: 6.243275122583751e-08\nDegrees of freedom for cuisine and price range: 460\nANOVA F-statistic: 1.6379146795766013\nP-value: 0.0001221911944649096"
  },
  {
    "objectID": "technical-details/data-cleaning/clean.html",
    "href": "technical-details/data-cleaning/clean.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "import pandas as pd\n\n# reading in the data\nyelp_zip = pd.read_csv(\"../../data/processed-data/yelp_zip.csv\")\nprint(yelp_zip.head(5))\n\n   Unnamed: 0 Median_Income  zip_code                              name  \\\n0           0       133,211     20001                    Compass Coffee   \n1           1       133,211     20001  Marianne’s by DC Central Kitchen   \n2           2       133,211     20001        Sankofa Video Books & Cafe   \n3           3       133,211     20001                 La Colombe Coffee   \n4           4       133,211     20001              Sylvan Cafe & Bakery   \n\n        cuisine price_range  rating  review_count   latitude  longitude  \n0  Coffee & Tea          $$     4.1          92.0  38.916256 -77.022773  \n1         Cafes           0     4.6          17.0  38.898720 -77.024770  \n2    Bookstores           $     4.5         167.0  38.925561 -77.023150  \n3  Coffee & Tea          $$     4.0         303.0  38.901051 -77.020103  \n4      Bakeries           $     4.0         134.0  38.915393 -77.012592  \n\n\n\n# removing the first column\ncols_to_drop = \"Unnamed: 0\"\nyelp_zip = yelp_zip.drop(columns=cols_to_drop)\n\n# numerizing the price_range variable:\n# used the avergae of the ranges given by Yelp\nprice_range_values = yelp_zip['price_range'].unique()\n\ndef price_conversion(price):\n    if price == \"$\":\n        return 5\n    elif price == \"$$\":\n        return 20.5\n    elif price == \"$$$\":\n        return 45.5\n    elif price == \"$$$$\":\n        return 61\n    else:\n        return 0\n    \nyelp_zip['price'] = yelp_zip['price_range'].apply(price_conversion)\n\nprint(yelp_zip.head(5))\n\n# check that the variables are correct type\nprint(yelp_zip.dtypes)\n\n# making median income numeric\nyelp_zip['Median_Income'] = yelp_zip['Median_Income'].str.replace(\",\", \"\")\nyelp_zip['Median_Income'] = pd.to_numeric(yelp_zip['Median_Income'])\n\nprint(yelp_zip.dtypes)\n\n  Median_Income  zip_code                              name       cuisine  \\\n0       133,211     20001                    Compass Coffee  Coffee & Tea   \n1       133,211     20001  Marianne’s by DC Central Kitchen         Cafes   \n2       133,211     20001        Sankofa Video Books & Cafe    Bookstores   \n3       133,211     20001                 La Colombe Coffee  Coffee & Tea   \n4       133,211     20001              Sylvan Cafe & Bakery      Bakeries   \n\n  price_range  rating  review_count   latitude  longitude  price  \n0          $$     4.1          92.0  38.916256 -77.022773   20.5  \n1           0     4.6          17.0  38.898720 -77.024770    0.0  \n2           $     4.5         167.0  38.925561 -77.023150    5.0  \n3          $$     4.0         303.0  38.901051 -77.020103   20.5  \n4           $     4.0         134.0  38.915393 -77.012592    5.0  \nMedian_Income     object\nzip_code           int64\nname              object\ncuisine           object\nprice_range       object\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\nprice            float64\ndtype: object\nMedian_Income      int64\nzip_code           int64\nname              object\ncuisine           object\nprice_range       object\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\nprice            float64\ndtype: object\n\n\n\n# investigating the cuisine variable\ncuisine_values = yelp_zip['cuisine'].unique()\nprint(cuisine_values)\n\n['Coffee & Tea' 'Cafes' 'Bookstores' 'Bakeries' 'Coffee Roasteries'\n 'Breakfast & Brunch' 'Italian' 'New American' 'French' 'Turkish' 'Bars'\n 'Sandwiches' 'Mexican' 'Peruvian' 'Pubs' 'Chinese' 'Barbeque' 'Greek'\n 'Seafood' 'Spanish' 'Indian' 'Thai' 'Latin American' 'Japanese' 'Ramen'\n 'Persian/Iranian' 'Dim Sum' 'Burgers' 'Korean' 'Sports Bars' 'American'\n 'Delis' 'Puerto Rican' 'Modern European' 'Salvadoran' 'Asian Fusion'\n 'Vegan' 'Tapas/Small Plates' 'Cocktail Bars' 'Speakeasies' 'Irish'\n 'Dive Bars' 'Beer Gardens' 'Dance Clubs' 'Lounges' 'Swimming Pools'\n 'Music Venues' 'Pool Halls' 'Wine Bars' 'Art Classes' 'Specialty Food'\n 'Bubble Tea' 'Lebanese' 'Pizza' 'Trinidadian' 'Laotian' 'Sushi Bars'\n 'Mediterranean' 'Steakhouses' 'Tacos' 'Cultural Center' 'Distilleries'\n 'Piano Bars' 'Breweries' 'Cheese Shops' 'Herbs & Spices'\n 'Beer, Wine & Spirits' 'Vietnamese' 'Southern' 'British' 'Golf'\n 'Mini Golf' 'Juice Bars & Smoothies' 'Cantonese' 'Bagels' 'Venezuelan'\n 'Whiskey Bars' 'Gay Bars' 'Performing Arts' 'Public Markets' 'Creperies'\n 'Pet Adoption' 'Banks & Credit Unions' 'Chocolatiers & Shops' 'Ukrainian'\n 'Noodles' 'Belgian' 'Jazz & Blues' 'Scandinavian' 'Themed Cafes'\n 'Desserts' 'Vinyl Records' 'Brazilian' 'Ethiopian' 'Brasseries'\n 'Moroccan' 'Caribbean' 'Afghan' 'Barbers' 'Beer Bar' 'Salad' 'Tiki Bars'\n 'Filipino' 'Malaysian' 'Hong Kong Style Cafe' 'Georgian'\n 'Himalayan/Nepalese' '0' 'Fish & Chips' 'Falafel' 'Karaoke' 'Szechuan'\n 'Egyptian' 'German' 'Pan Asian' 'Shared Office Spaces' 'Irish Pub']\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Missing Values Handling:\n# Finding missing values\nmissing_values = yelp_zip.isnull().sum()\nprint(missing_values)\n\n# count of missing values in price (zeros)\nprice_counts = yelp_zip['price'].value_counts()\nprint(price_counts)\n\n# count of missing values in income (zeros)\nprice_counts = yelp_zip['Median_Income'].value_counts()\nprint(price_counts)\n\n# count of missing values in zipcode (zeros)\nprice_counts = yelp_zip['zip_code'].value_counts()\nprint(price_counts)\n\n# count of missing values in review number (zeros)\nprice_counts = yelp_zip['review_count'].value_counts()\nprint(price_counts)\n\n# count of missing values in rating (zeros)\nprice_counts = yelp_zip['rating'].value_counts()\nprint(price_counts)\n\n# plot of missing rating values\n# Scatter plot of restaurant names and ratings\nplt.figure(figsize=(12, 6))\nsns.scatterplot(data=yelp_zip, x='name', y='rating', hue=(yelp_zip['rating'] == 0), palette={True: 'red', False: 'blue'})\nplt.xticks(rotation=90, fontsize=0)  # Rotate x-axis labels and set font size\nplt.title('Scatter plot of Missing Ratings by Restaurant')\nplt.xlabel('Restaurant Names')\nplt.ylabel('Ratings')\nplt.legend(title='Rating is Zero', loc='upper right', labels=['Non-zero', 'Zero'])\nplt.show()\n\n# dropping the rows that have a zero for 'rating'\nyelp_zip = yelp_zip[yelp_zip['rating'] != 0]\n\nprice_counts = yelp_zip['rating'].value_counts()\nprint(price_counts)\n\n# plot after fixing the missing ratings\nplt.figure(figsize=(12, 6))\nsns.scatterplot(data=yelp_zip, x='name', y='rating', hue=(yelp_zip['rating'] == 0), palette={True: 'red', False: 'blue'})\nplt.xticks(rotation=90, fontsize=0)  # Rotate x-axis labels and set font size\nplt.title('Scatter plot of Ratings by Restaurant After Handling Missing Values')\nplt.xlabel('Restaurant Names')\nplt.ylabel('Ratings')\nplt.legend(title='Rating is Zero', loc='upper right', labels=['Non-zero', 'Zero'])\nplt.show()\n\n# no important missing values found besides in the ratings variable which were removed\n\nMedian_Income    0\nzip_code         0\nname             0\ncuisine          0\nprice_range      0\nrating           0\nreview_count     0\nlatitude         0\nlongitude        0\nprice            0\ndtype: int64\nprice\n20.5    387\n0.0     247\n45.5     59\n5.0      52\n61.0      7\nName: count, dtype: int64\nMedian_Income\n133211    135\n107130     94\n132374     90\n109147     68\n0          66\n155054     43\n145048     43\n152955     38\n97694      33\n106560     33\n106930     30\n34352      19\n123134     15\n94820      13\n97327      11\n97507       9\n169489      5\n48106       2\n235511      1\n110375      1\n53394       1\n87552       1\n47871       1\nName: count, dtype: int64\nzip_code\n20001    135\n20002     94\n20009     90\n20005     68\n20007     43\n20003     43\n20004     38\n20010     33\n20024     33\n20036     30\n20006     19\n20008     15\n20037     13\n20011     11\n20017      9\n20910      6\n22201      6\n20016      5\n22202      4\n22209      3\n22314      3\n22203      3\n20814      3\n20020      2\n22003      2\n20852      2\n20052      2\n20912      2\n20250      2\n20735      2\n20781      2\n20000      2\n20560      2\n22101      2\n20740      2\n20019      1\n20012      1\n20032      1\n20015      1\n20018      1\n22204      1\n22205      1\n20057      1\n20303      1\n20472      1\n22304      1\n20902      1\n20737      1\n20815      1\n20056      1\n20045      1\n22301      1\n20064      1\n20745      1\n22041      1\n22305      1\nName: count, dtype: int64\nreview_count\n56.0      11\n11.0       9\n2.0        9\n57.0       8\n7.0        8\n          ..\n360.0      1\n1032.0     1\n1128.0     1\n286.0      1\n804.0      1\nName: count, Length: 400, dtype: int64\nrating\n4.3    99\n4.1    76\n4.4    72\n4.2    71\n4.0    71\n3.9    55\n4.5    52\n4.6    44\n3.8    44\n4.8    27\n4.7    22\n3.4    19\n3.5    16\n3.3    15\n4.9    14\n5.0    14\n3.6    13\n3.7    12\n2.9     5\n0.0     3\n3.0     3\n3.1     2\n3.2     2\n2.6     1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nrating\n4.3    99\n4.1    76\n4.4    72\n4.2    71\n4.0    71\n3.9    55\n4.5    52\n4.6    44\n3.8    44\n4.8    27\n4.7    22\n3.4    19\n3.5    16\n3.3    15\n5.0    14\n4.9    14\n3.6    13\n3.7    12\n2.9     5\n3.0     3\n3.1     2\n3.2     2\n2.6     1\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\n\n# Outlier Detection using Z-Score\n\n# Median Income (Median_Income)\nyelp_zip['income_zscore'] = stats.zscore(yelp_zip['Median_Income'])\nzscore_outliers = yelp_zip[(yelp_zip['income_zscore'] &lt; -3) | (yelp_zip['income_zscore'] &gt; 3)]\nprint(\"Z-score income outliers:\", zscore_outliers)\n# yields one outlier: Open City at the National Cathedral in zip: 20015\n\n# # Rating (rating)\nyelp_zip['rating_zscore'] = stats.zscore(yelp_zip['rating'])\nzscore_outliers = yelp_zip[(yelp_zip['rating_zscore'] &lt; -3) | (yelp_zip['rating_zscore'] &gt; 3)]\nprint(\"Z-score rating outliers:\", zscore_outliers)\n# 6 rating outliers all with scores lower than 3.0\n\n# # Review Count (review_count)\nyelp_zip['review_zscore'] = stats.zscore(yelp_zip['review_count'])\nzscore_outliers = yelp_zip[(yelp_zip['review_zscore'] &lt; -3) | (yelp_zip['review_zscore'] &gt; 3)]\nprint(\"Z-score review outliers:\", zscore_outliers)\n# yields 12 outliers all of which have more ratings than the other places\n# there are no overlapping outliers, so leave for now but may need to remove the ratings outliers\n# for our purposes the review_count outliers are removed after visualiatization:\n\n# Visualizing the Outliers with boxplots\n\n# Median Income Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['Median_Income'])\nplt.title('Boxplot of Median Income')\nplt.xlabel('Median Income')\nplt.show()\n\n# Ratings Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['rating'])\nplt.title('Boxplot of Yelp Ratings')\nplt.xlabel('Rating')\nplt.show()\n\n# Review count Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['review_count'])\nplt.title('Boxplot of Review Counts')\nplt.xlabel('Review Count')\nplt.show()\n\n# Price Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=yelp_zip['price'])\nplt.title('Boxplot of Price')\nplt.xlabel('price')\nplt.show()\n\n# remove the review count outliers:\nyelp_zip = yelp_zip[(yelp_zip['review_zscore'] &gt;= -3) & (yelp_zip['review_zscore'] &lt;= 3)]\n\ncols_to_drop = ['rating_zscore', 'review_zscore', 'income_zscore']\nyelp_zip = yelp_zip.drop(columns=cols_to_drop)\n\nZ-score income outliers:      Median_Income  zip_code                                 name cuisine  \\\n590         235511     20015  Open City At the National Cathedral   Cafes   \n\n    price_range  rating  review_count   latitude  longitude  price  \\\n590          $$     3.8         150.0  38.929811 -77.071355   20.5   \n\n     income_zscore  \n590       3.003991  \nZ-score rating outliers:      Median_Income  zip_code                         name       cuisine  \\\n20          133211     20001                Corner Bakery      Bakeries   \n105         133211     20001             Medusa lounge Dc       Lounges   \n451         123134     20008                  Harry's Pub          Pubs   \n452         123134     20008                  Harry's Pub          Pubs   \n615          97694     20024  Casey's Coffee & Sandwiches  Coffee & Tea   \n669         106930     20036                  The Rooftop       Lounges   \n\n    price_range  rating  review_count   latitude  longitude  price  \\\n20           $$     2.9         205.0  38.896568 -77.009497   20.5   \n105           0     2.6           5.0  38.925010 -77.022910    0.0   \n451          $$     2.9          57.0  38.924394 -77.054888   20.5   \n452          $$     2.9          57.0  38.924394 -77.054888   20.5   \n615          $$     2.9          25.0  38.883440 -77.016027   20.5   \n669         $$$     2.9         104.0  38.910810 -77.045650   45.5   \n\n     income_zscore  rating_zscore  \n20        0.540104      -3.164998  \n105       0.540104      -3.910543  \n451       0.297400      -3.164998  \n452       0.297400      -3.164998  \n615      -0.315320      -3.164998  \n669      -0.092872      -3.164998  \nZ-score review outliers:      Median_Income  zip_code                               name       cuisine  \\\n42          133211     20001                           Zaytinya         Greek   \n71          133211     20001                 Daikaya Ramen Shop         Ramen   \n285         152955     20004                             Rasika        Indian   \n338         109147     20005                   Old Ebbitt Grill          Bars   \n342         109147     20005                       The Hamilton       Seafood   \n361         109147     20005                   Old Ebbitt Grill          Bars   \n363         109147     20005                       The Hamilton       Seafood   \n389          34352     20006      Founding Farmers - Washington      American   \n405         145048     20007                      Baked & Wired      Bakeries   \n421         145048     20007  Founding Farmers Fishers & Bakers  New American   \n423         145048     20007                          il Canale       Italian   \n499         132374     20009                       Le Diplomate    Brasseries   \n\n    price_range  rating  review_count   latitude  longitude  price  \\\n42          $$$     4.2        6001.0  38.899040 -77.023490   45.5   \n71           $$     4.1        3675.0  38.898600 -77.019590   20.5   \n285          $$     4.2        3943.0  38.895008 -77.021286   20.5   \n338          $$     4.2       11058.0  38.897967 -77.033342   20.5   \n342          $$     3.9        3686.0  38.897454 -77.032080   20.5   \n361          $$     4.2       11059.0  38.897967 -77.033342   20.5   \n363          $$     3.9        3686.0  38.897454 -77.032080   20.5   \n389          $$     4.0       17388.0  38.900366 -77.044435   20.5   \n405          $$     4.4        5248.0  38.903913 -77.060248   20.5   \n421          $$     3.8        4913.0  38.901784 -77.059733   20.5   \n423          $$     4.3        4908.0  38.904493 -77.060974   20.5   \n499         $$$     4.3        5414.0  38.911350 -77.031570   45.5   \n\n     income_zscore  rating_zscore  review_zscore  \n42        0.540104       0.065696       5.252002  \n71        0.540104      -0.182819       3.061893  \n285       1.015637       0.065696       3.314236  \n338      -0.039475       0.065696      10.013559  \n342      -0.039475      -0.679849       3.072250  \n361      -0.039475       0.065696      10.014501  \n363      -0.039475      -0.679849       3.072250  \n389      -1.840907      -0.431334      15.973745  \n405       0.825197       0.562725       4.542994  \n421       0.825197      -0.928364       4.227566  \n423       0.825197       0.314210       4.222858  \n499       0.519945       0.314210       4.699296  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Reviewing Data Types Again\n\nprint(yelp_zip.dtypes)\n\n# already made the price variable numeric and the median_income variable numeric\n# encoding the price_range variable (number is the amount of dollar signs)\n# just needed something numeric for price\ndef price_conversion(price):\n    if price == \"$\":\n        return 1\n    elif price == \"$$\":\n        return 2\n    elif price == \"$$$\":\n        return 3\n    elif price == \"$$$$\":\n        return 4\n    else:\n        return 0\n    \nyelp_zip['price_range'] = yelp_zip['price_range'].apply(price_conversion)\nprint(yelp_zip['price_range'])\n\nMedian_Income      int64\nzip_code           int64\nname              object\ncuisine           object\nprice_range       object\nrating           float64\nreview_count     float64\nlatitude         float64\nlongitude        float64\nprice            float64\ndtype: object\n0      2\n1      0\n2      1\n3      2\n4      1\n      ..\n747    0\n748    0\n749    0\n750    0\n751    0\nName: price_range, Length: 737, dtype: int64\n\n\n\n# Normalization and Scaling\n\n# checking the skewness of the numeric variables\nskewness = yelp_zip[['Median_Income', 'price_range', 'rating', 'review_count', 'latitude', 'longitude', 'price']].skew()\nprint(skewness)\n\n# median income and review_count are highly skewed\n# plotting the skewed distributions\nplt.figure(figsize=(10, 6))\nsns.histplot(yelp_zip['Median_Income'], kde=True)\nplt.title('Distribution of Median Income')\nplt.xlabel('Median Income')\nplt.ylabel('Frequency')\nplt.show()\n# negatively skewed\n\nplt.figure(figsize=(10, 6))\nsns.histplot(yelp_zip['review_count'], kde=True)\nplt.title('Distribution of Review Count')\nplt.xlabel('Review Count')\nplt.ylabel('Frequency')\nplt.show()\n# positively skewed\n\nMedian_Income   -1.586693\nprice_range     -0.170882\nrating          -0.521142\nreview_count     2.903098\nlatitude         0.551364\nlongitude       -0.934695\nprice            0.790753\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# outputting the cleaned dataset\nyelp_zip.to_csv(\"clean_yelp_zip.csv\")"
  },
  {
    "objectID": "index.html#food-for-thought-investigating-the-socioeconmic-food-gap-in-the-d.c.-area",
    "href": "index.html#food-for-thought-investigating-the-socioeconmic-food-gap-in-the-d.c.-area",
    "title": "Landing page",
    "section": "",
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\n\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInclude your data science questions on this page.\n\n\n\n\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\n\n\n\n\n\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "technical-details/eda/main.html#summary-and-interpretaion-of-results",
    "href": "technical-details/eda/main.html#summary-and-interpretaion-of-results",
    "title": "Exploratory Data Analysis",
    "section": "Summary and Interpretaion of Results",
    "text": "Summary and Interpretaion of Results\nFollowing extensive exploratory data analysis and hypothesis testing, we have some important and interesting findings. For starters, we found that most restaurnats in DC are found in the zipcodes 20001, 20002, and 20009 and that restaurants in DC are overall higher than average with all scores above a 3.0 rating. There are also a large portion of restaurants and bars in the DC area and most of the restaurants fall in the $11-30 per meal per person range. We then saw a slight positive correlation between the number of yelp views and the price range of the restaurant. Interestingly, there seemed to be no significant correlations between the yelp rating and any of the other variables. The highest ratings, however, went to bars and the lowest went to bars and cafes. We then attempted to statistically investigate these findings. This yielded a non-significant correlation between median income and yelp rating. There was, however, a positive and significant correlation between the review count and the price of the restaurant, following what was seen previously. There was an insignificant correlation between the review count and the yelp rating, which is a good sanity check that simply having more reviews leads to a skewed and biased rating. Finally, the mean rating across the cuisine category and the the mean income across the price ranges were significantly different. Taken together, these results lead to some interpretations. For one, it seems customers are more likely to leave a review on a restaurant if they paid more for the experience, indicating price might make a person more hyperaware of their opinion of a restaurant. Additionally, it is clearly hard to predict a Yelp score based solely on some basic information about the restaurant, there is clearly a lot more nuance about what constitutes a good yelp score, and further a good restaurant. Finally, it seems that better neighborhoods don’t necessarily have better restaurants, at least for the DC areas and according to the people spending the time to leave Yelp reviews."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dimensionality-reduction",
    "href": "technical-details/unsupervised-learning/main.html#dimensionality-reduction",
    "title": "Unsupervised Learning",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nPCA\n\n# Principal Component Analysis (PCA)\n\n# pre-processing to use data in PCA\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\npca = PCA(n_components=8) # initiating a pca \nyelp_pca = pca.fit_transform(yelp_scaled) # reducing dimensionality\n\n# explained variance to determine optimal number of features:\ndef plot_variance_explained(pca):\n    \"\"\"helper function to plot explained variance from lab 4.2\"\"\"\n    # visualize variance explained by PCA components\n    \n    print(pca.explained_variance_ratio_[0:10])\n    print(np.cumsum(pca.explained_variance_ratio_[0:10]))\n\n    # plot explained variance ratio\n    fig, ax = plt.subplots()\n    ax.plot(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, marker='o')\n    ax.set(xlabel=\"number of components\", ylabel=\"explained variance ratio\")\n    plt.show()\n\n    # plot cumulative sum of explained variance\n    fig, ax = plt.subplots()\n    ax.plot(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n    ax.set(xlabel=\"number of components\", ylabel=\"cumulative explained variance\")\n    plt.show()\n    \nplot_variance_explained(pca)\n\n[0.26763243 0.18944621 0.15147403 0.13711514 0.11630842 0.08958895\n 0.04648898 0.00194583]\n[0.26763243 0.45707864 0.60855267 0.74566781 0.86197623 0.95156518\n 0.99805417 1.        ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we have plotted the explained variance versus the number of components and the cumulative explained variance versus the number of components. These plots together are meant to help choose the number of components to utilize in intializing a PCA object. Generally, we look for an “elbow point” in the first plot, where the explained variance begins to taper off, however, the plot lacks a true elbow point, thus, we will look mostly at the cumulative explained variance plot for out decision. Based on the first plot, we are looking somewhere in the 4-7 components, range and then we must decide how we want to balance the total variance retained and the model complexity. We are going to decide to choose 6 components to retain about 95% of the variance and proceed with the principal component analysis utilizing this number.\n\ndef plot_2D(X,color_vector):\n    \"\"\"Utility plotting function taken from lab 4.2\"\"\"\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='PC-1 ', ylabel='PC-2',\n    title='Principal Component Analysis results')\n    ax.grid()\n    plt.show()\n    \npca = PCA(n_components=6) # initiating a pca \nyelp_pca = pca.fit_transform(yelp_scaled) # reducing dimensionality\n\ncuisine_colors = yelp_zip['cuisine_cat'].astype('category').cat.codes # converting cuisine to categorical codes\nincome_colors = yelp_zip['Median_Income'].astype('category').cat.codes\nrating_colors = yelp_zip['rating'].astype('category').cat.codes\nreview_colors = yelp_zip['review_count'].astype('category').cat.codes\nzip_colors = yelp_zip['zip_code'].astype('category').cat.codes\nprice_colors = yelp_zip['price_range'].astype('category').cat.codes\n\n\n# plotting the PCA color coded by the Income values\nplot_2D(yelp_pca, income_colors)\n\n\n\n\n\n\n\n\nIn the above plot, we are showing the PCA results with color-coding of the median income levels. We see that there are potential clusters like the light green at the bottom of the plot and the large cluster at (0,1). However, there is a large amount of overlap in the clusters and no clear distribution of the points, indicating that is likely that the influence of cuisine types on the PCA captured patterns is not meaningul. There seems to be no clear strong correlation between the features of the restaurant (median income of area, zipcode, price range, yelp rating, review count, latitude, and longitude) and the income of the surronding area.\n\n# plotting the PCA color coded by the Yelp ratings\nplot_2D(yelp_pca, rating_colors)\n\n\n\n\n\n\n\n\nWe then tried to visualize the same PCA with the yelp ratings as the color-coding. Again, we see there is a large amount of overlap in the clusters and almost no clear clusters that are separate from the other clusters. This indicating that it is likely that the influence on the PCA captured patterns is not meaningul for yelp ratings. Again, there is no clear correlation between the features of the restaurant and the yelp rating that the restaurant receives.\n\n# plotting the PCA color coded by the number of reviews\nplot_2D(yelp_pca, review_colors)\n\n\n\n\n\n\n\n\nNext, we again with visualizing the PCA with the number of reviews as the color-coding. We see that there is somewhat of a striping pattern here where the colors seem to aggregate in a vertical fashion. However, these clusters appear at each of the x-coordinates of the plot, making the pattern less clear. There is still a decent amount of overlap, thus we cannot fully conclude either way if there is a correlation between the features of the restaurant and the number of reviews that the restaurant receives.\n\n# plotting the PCA color coded by the cusine categories\nplot_2D(yelp_pca, cuisine_colors)\n\n\n\n\n\n\n\n\nFinally, we color-code the PCA results with the cuisine categories. We see that there is almost complete overlap between clusters and no clear distribution of the points, indicating that is likely that the influence of cuisine types on the PCA captured patterns is not meaningul. We can conclude from the plot that there is no strong correlation between the features of the restaurant and the type of cuisine the restaurant is.\n\n\nt-SNE\n\n# T-SNE \n# with cuisine\ndef plot_2D(X,color_vector):\n    \"\"\"Utility plotting function taken from lab 4.2, reusing for t-SNE\"\"\"\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='t-SNE Dimension 1', ylabel='t-SNE Dimension 2',\n    title='t-SNE Analysis results')\n    ax.grid()\n    plt.show()\n\ntsne = TSNE(n_components=2) # initiating tsne\nyelp_tsne = tsne.fit_transform(yelp_scaled)\nplot_2D(yelp_tsne, cuisine_colors)\n\n\n\n\n\n\n\n\nThe above plot is the results of the t-SNE process, which as was said in the introduction, attempts to visualize high-dimensional data in a low-dimensional space. The plot shows that t-SNE produced no distinct clusters. Here the colors are coded by the cuisine types and we see that the points are clearly dispersed throughout the plot in no clear patterns. There is a lot of overlap between points, which indicates that the cuisine types are not clearly distinct from each other in terms of the restaurant features that make up the types.\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'rating']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\n\ntsne = TSNE(n_components=2) # initiating tsne\nyelp_tsne = tsne.fit_transform(yelp_scaled)\nplot_2D(yelp_tsne, rating_colors)\n\n\n\n\n\n\n\n\nThis t-SNE plot is utilizing the Yelp ratings for color coding. The points are distributed throughout the plot in no clear patterns. Again, there is a large amount of overlap between points, which indicates that the Yelp ratings are distributed across the restaurant features pretty randomly.\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'review_count']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\n\ntsne = TSNE(n_components=2) # initiating tsne\nyelp_tsne = tsne.fit_transform(yelp_scaled)\nplot_2D(yelp_tsne, review_colors)\n\n\n\n\n\n\n\n\nWe attempted another t-SNE plot is with the review counts for color coding. Again, we see no clear clustering patterns amoung the points and there is a lot of overlap between the coloring. We are seeing that the review count is also not strongly related to the features of the restaurant.\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'price_range', 'price']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\n\ntsne = TSNE(n_components=2) # initiating tsne\nyelp_tsne = tsne.fit_transform(yelp_scaled)\nplot_2D(yelp_tsne, price_colors)\n\n\n\n\n\n\n\n\nNext, we attempted again the t-SNE plot, but with the price ranges for color coding. Again, we see no clear clustering patterns amoung the points and there is a lot of overlap between the coloring. We are seeing that the price range of a restaurant is also not strongly related to the features.\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'Median_Income']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\n\ntsne = TSNE(n_components=2) # initiating tsne\nyelp_tsne = tsne.fit_transform(yelp_scaled)\nplot_2D(yelp_tsne, income_colors)\n\n\n\n\n\n\n\n\nIn our last t-SNE attempt, we plotted the t-SNE results with the Median Income values of the zipcode where the restaurnt is located as the colors. This lead to somewhat more interesting results as there is less overlap and some potential loose clustering occurring. We can see that there is a teal clutser around (-10, 20) that may be a distinct cluster, however, there are some outliers for this grouping. There is also a light green cluster situated to the right of this cluster, again that has a pattern, but with outliers. The rest of the clusters are mostly overlapping and dispersed throughout the plot. This leads us to believe that there may be some relation between the median income and the features of the restaurant, however, in specific cases and not across the board for all restaurants and income brackets.\nWe now played around with the perplexity values for the t-SNE results, utilizing perplexity of 5, 30, and 50. The perplexity is used to manage the balance of the global and local structures within the data. Lower values of perplexity will focus more on the local structure, while a higher perplexity will focus more on the global structure. These will be visualized with color codings for Yelp rating and Median Income. These two were choosen because Yelp rating is most aligned with our reserach questions of understanding what makes us a yelp rating and because median income showed the most preliminary clustering in the above plots.\n\n# experimenting with different perplexity values\n# perplexity = 5\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'Median_Income']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_5 = TSNE(n_components=2, perplexity=5)\nyelp_tsne5 = tsne_5.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne5, income_colors)\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'rating']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_5 = TSNE(n_components=2, perplexity=5)\nyelp_tsne5 = tsne_5.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne5, rating_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the two plots above, we have plotted the t-SNE results utilizing a perplexity value of 5 (as opposed to the default value of 30). In the first plot, we color code with the median income values and the second plot is color coded with the yelp ratings. In both of the plots, we see that the points are distributed across the plots in no clear pattern. There is a large amount of overlap and no emerging clusters, thus we can say that there is no relation between the features of the restaurant and the median income and yelp rating when looking at the local structure of the dataset.\n\n# perplexity = 35\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'Median_Income']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_35 = TSNE(n_components=2, perplexity=35)\nyelp_tsne35 = tsne_35.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne35, income_colors)\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'rating']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_35 = TSNE(n_components=2, perplexity=35)\nyelp_tsne35 = tsne_35.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne35, rating_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe then plotted the t-SNE results utilizing a perplexity value of 35 (which is still relatively close to the default value of 30). Again, the plots are color coded by median income values and yelp ratings, respectively. In the second plot, we again see that the points are distributed across the plots in no clear pattern and there are no emerging clusters, thus we can say that there is no relation between the features of the restaurant and yelp rating when looking more at the global structure of the data. However, looking at teh median income plot, we see the light green and teal clusters near (-10, 15), indicating there may be some emerging relation between the features of the restaurant and the income of the neighborhood.\n\n# experimenting with different perplexity values\n# perplexity = 50\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'Median_Income']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_50 = TSNE(n_components=2, perplexity=50)\nyelp_tsne50 = tsne_50.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne50, income_colors)\n\nnumeric_yelp = yelp_zip.drop(columns=['name', 'cuisine', 'cuisine_cat', 'rating']) # keeping only numeric variables\nscaler = StandardScaler() # initializing a scaler\nyelp_scaled = scaler.fit_transform(numeric_yelp) # standardizing the variables\ntsne_50 = TSNE(n_components=2, perplexity=50)\nyelp_tsne50 = tsne_50.fit_transform(yelp_scaled)\n\nplot_2D(yelp_tsne50, rating_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe finally plot the t-SNE results utilizing a perplexity value of 50 (which is more than the 30 and 35 and will look at the global structure of the data). Again, the plots are color coded by median income values and yelp ratings, respectively. In the second plot, we again see that the points are distributed across the plots in no clear pattern and there are no emerging clusters, thus we can say that there is no relation between the features of the restaurant and yelp rating when looking more at the global structure of the data. However, looking at the median income plot, we see the light green and teal clusters near (-10, 15) and are also seeing an emerging loose cluster of green near (0,10). This could again indicate there may be some emerging relation between the features of the restaurant and the income of the neighborhood, especially when we account for the global structure of the data and look at more broad relationships between points.\n\n\nComparison\nWhen we performed the principal component analysis and t-SNE, we overwhelming, found little to no significant results. The PCA yielded that there was no correlation between the features of the restaurant and the median income level, the yelp rating, the number of yelp reviews, and the cuisine type. The t-SNE results backed up this finding by making clear that there was no relationship between the restaurant features and the cuisine type, the yelp rating, the number of reviews, and the price range. T-SNE did, however, begin to show a slight clustering in the relationship between the features of the restaurant and the median income level of the surronding area. This clustering was extremely slight and there was still alot of overlapping and outliers. This clustering began to appear more as the perplexity value increased, however, this pattern may be explained by the relationship between median income and zip code and was too slight to make a true conclusion about the relationship. The small difference in the result is potentially due to the bias from the zip code or from the difference in the technique of PCA and t-SNE. T-SNE is a non-linear method, so it may have been able to find the small relationship in the between income and the features because it was more apparent in the global structure, which was seen as the perplexity value increased. Overall, we come to no true conclusions about the relationship between the features of the restaurant."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#clustering-methods",
    "href": "technical-details/unsupervised-learning/main.html#clustering-methods",
    "title": "Unsupervised Learning",
    "section": "Clustering Methods",
    "text": "Clustering Methods\n\n# pre-processing steps\n\n# scaling the numeric features\nscaler = StandardScaler() # initializing a scaler\nscaled_nums = scaler.fit_transform(yelp_zip[['Median_Income', 'zip_code', 'price_range', 'rating', 'review_count', 'latitude', 'longitude', 'price']])\nyelp_zip[['Median_Income', 'zip_code', 'price_range', 'rating', 'review_count', 'latitude', 'longitude', 'price']] = scaled_nums\n\nprint(yelp_zip.head(5))\nyelp_zip = yelp_zip.drop(columns = ['name', 'cuisine', 'cuisine_cat'])\n\n   Median_Income  zip_code                        name       cuisine  \\\n0       0.488229 -0.731905              Compass Coffee  Coffee & Tea   \n1       0.488229 -0.731905  Sankofa Video Books & Cafe    Bookstores   \n2       0.488229 -0.731905           La Colombe Coffee  Coffee & Tea   \n3       0.488229 -0.731905        Sylvan Cafe & Bakery      Bakeries   \n4       0.488229 -0.731905                 Urban Roast  Coffee & Tea   \n\n   price_range    rating  review_count  latitude  longitude     price  \\\n0    -0.120364  0.023268     -0.653447  0.673732   0.156349 -0.208398   \n1    -2.054779  1.125458     -0.530862  1.299282   0.137330 -1.642612   \n2    -0.120364 -0.252279     -0.308575 -0.348487   0.291018 -0.208398   \n3    -2.054779 -0.252279     -0.584799  0.615718   0.669956 -1.642612   \n4    -0.120364 -1.905564      0.701521 -0.553228   0.048241 -0.208398   \n\n  cuisine_cat  \n0       cafes  \n1       cafes  \n2       cafes  \n3       cafes  \n4       cafes  \n\n\n\n# checking for highly correlated features\nsns.pairplot(yelp_zip)\n\n\n\n\n\n\n\n\nThere are no clear correlated features that need to be removed for the further analysis.\n\n# checking the values are standardized\nprint(yelp_zip.head(10))\n\n   Median_Income  zip_code  price_range    rating  review_count  latitude  \\\n0       0.488229 -0.731905    -0.120364  0.023268     -0.653447  0.673732   \n1       0.488229 -0.731905    -2.054779  1.125458     -0.530862  1.299282   \n2       0.488229 -0.731905    -0.120364 -0.252279     -0.308575 -0.348487   \n3       0.488229 -0.731905    -2.054779 -0.252279     -0.584799  0.615718   \n4       0.488229 -0.731905    -0.120364 -1.905564      0.701521 -0.553228   \n5       0.488229 -0.731905    -0.120364 -2.181111      0.415491  0.433059   \n6       0.488229 -0.731905    -0.120364  0.574363      0.101674  0.015760   \n7       0.488229 -0.731905    -0.120364  0.849911      3.354251 -0.253399   \n8       0.488229 -0.731905    -0.120364  0.298816     -0.256273 -0.375423   \n9       0.488229 -0.731905    -0.120364  0.298816     -0.674695  0.283428   \n\n   longitude     price  \n0   0.156349 -0.208398  \n1   0.137330 -1.642612  \n2   0.291018 -0.208398  \n3   0.669956 -1.642612  \n4   0.048241 -0.208398  \n5   0.687199 -0.208398  \n6   0.047625 -0.208398  \n7   0.433704 -0.208398  \n8  -0.043970 -0.208398  \n9  -0.043504 -0.208398  \n\n\n\nK-Means\n\n# K-Means\n\ndef kmean_fit(k): # kmean function \n    \"\"\"KMean funciton taken from lab 4.1\"\"\"\n    global yelp_zip, model\n    model=KMeans(n_clusters=k,max_iter=5000,tol=1e-6)\n    model.fit(yelp_zip)\n    return model.inertia_\n\nks=[]\ninertias=[]\nfor k in range(1,20):\n    inertia=kmean_fit(k)\n    ks.append(k)\n    inertias.append(inertia)\n\nplt1 = plt.plot(ks,inertias,\"-o\")\nplt.title('Intertia versus number of clusters (elbow method)')\nplt.xlabel('k')\nplt.ylabel('inertia')\nplt.show(plt1)\n\n### Silhouette Score\nrange_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] # different possible number of clusters\nsil_scores = [] # initalizing empty list for silhouette scores\n\nfor n_clusters in range_n_clusters: # looping through the number of clusters\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10) # using Kmeans with each cluster value\n    cluster_labels = clusterer.fit_predict(yelp_zip)  # getting the labels\n    silhouette_avg = silhouette_score(yelp_zip, cluster_labels) # getting the silhoeutte score\n    sil_scores.append(silhouette_avg)\n\nplt2 = plt.plot(range_n_clusters, sil_scores, \"-o\", color='orange') # plotting number of clusters and silhouette score\nplt.title('Silhouette Scores versus number of clusters')\nplt.xlabel('k')\nplt.ylabel('silhouette score')\nplt.show(plt2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove we have plotted the inertia versus the number of clusters and the silhouette score versus the number of clusters. These plots together are meant to help choose the number of clusters to utilize in intializing k-Means. Generally, we look for an “elbow point” in the first plot, where the inertia begins to taper off, however, the plot lacks a true elbow point, thus, we will look mostly at the silhoeutte score for the decision. Based on the first plot, we are looking somewhere in the 5-12 cluster range. We are going to decide to choose 7 clusters to obtain a relatively high silhouette score.\n\n# Initializing K Means with optimal k value\nk = 7 # based on the silhouette scores\nmodel = KMeans(n_clusters=k, max_iter=5000, tol=1e-6)\nmodel.fit(yelp_zip)\nlabels = model.predict(yelp_zip)\n\n# Scatter plot of the clusters using ratings and median income\nplt.scatter(yelp_zip['rating'], yelp_zip['Median_Income'], c=labels, cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Median Income')\nplt.show()\n\n\n\n\n\n\n\n\nIn the above plot we see the yelp ratings plotted against the median income with K-Means clustering. The plot shows no clear clusters, lots of overlap, and points that are dispersed across both axes. This indicates that there is no strong correlation between the yelp rating and the median income of the area.\n\n# Scatter plot of the clusters using ratings and price range\nplt.scatter(yelp_zip['rating'], yelp_zip['price_range'], c=labels, cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Price Range')\nplt.show()\n\n\n\n\n\n\n\n\nIn the above plot we see the yelp ratings plotted against the price ranges with K-Means clustering. The plot shows no clear clusters, lots of overlap, and points that are dispersed across both axes. This indicates that there is no strong correlation between the yelp rating and the price range of the restaurant.\n\n# Scatter plot of the clusters using ratings and review count\nplt.scatter(yelp_zip['rating'], yelp_zip['review_count'], c=labels, cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n\n\n\n\n\n\n\nIn the above plot we see the yelp rating plotted against the number of reviews with K-Means clustering. The plot shows only one potential cluster, which is the green color towards the top of the graph. This would point to more reviews having highing rating, but it is not a strong cluster. Other than that there are no clear clusters and the points that are dispersed across both axes. This indicates that there is small potential for some correlation between the yelp rating and the number of reviews, but in a small subset and not across the board.\n\n# Scatter plot of the clusters using review count and price range\nplt.scatter(yelp_zip['review_count'], yelp_zip['price_range'], c=labels, cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Number of Yelp Reviews')\nplt.ylabel('Price Range')\nplt.show()\n\n\n\n\n\n\n\n\nIn the above plot we see the price range plotted against the number of Yelp reviews with K-Means clustering. The plot shows no clear clusters and the points are very dispersed across both axes. This indicates that there is no strong correlation between the price range of the restaurant and the number of yelp reviews.\n\n# Scatter plot of the clusters using price range and median income\nplt.scatter(yelp_zip['price_range'], yelp_zip['Median_Income'], c=labels, cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Price Range')\nplt.ylabel('Median Income')\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we plotted the median income against the price range utilizing K-Means clustering. The plot again shows no clear clusters and the points are very dispersed across both axes. This indicates that there is no strong correlation between the price range of the restaurant and the median income.\n\n# used CHATGPT for help with 3D plot\nyelp_clustered = pd.DataFrame(yelp_zip, columns=['rating', 'price_range', 'review_count'])\nyelp_clustered['cluster'] = labels\n\nfig = px.scatter_3d(\n    yelp_clustered,\n    x='rating',                # X-axis\n    y='price_range',           # Y-axis\n    z='review_count',          # Z-axis\n    color='cluster',           # Color points by cluster\n    symbol='cluster',          # Optional: Different symbols for each cluster\n    hover_data=['rating', 'price_range', 'review_count'],  # Show details on hover\n    title='Interactive 3D Clustering of Yelp Data'\n)\n\nfig.update_traces(marker=dict(size=5))  # Adjust marker size\nfig.update_layout(scene=dict(\n    xaxis_title='Rating',\n    yaxis_title='Price Range',\n    zaxis_title='Review Count'\n))\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIn the above plot we try to utilize a 3D plot to visualize the review count, price range and the rating see the price range with K-Means clustering. The plots shows a potential cluster of the purple x’s which appear towards the top of the plot. This again, points to a potential relationship between the number of reviews and the yelp rating of the restaurant for the two middle price ranges. Again, this lends to a loose conclusion about having more reviews having a slight positive effect on the yelp rating. the other clusters are relatively less strong and are more overlapping.\n\n\nDBSCAN\n\n# DBSCAN\n# modified from lab 4.1\n\nrange_eps= [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # possible values of epsilon (hyperparameter)\nrange_samples = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # possible number of samples (hyperparameter)\nsil_scores = [] # silhouette scores\nmax = -1 # initiailizing a max value\neps_and_sample = []\n\nfor epsilon in range_eps: # iterating through both of the hyperparamters\n    for sample in range_samples:\n        clustering = DBSCAN(eps=epsilon, min_samples=sample).fit(yelp_zip) # using DBSCAN\n        cluster_labels = clustering.labels_  # getting the cluster values\n        if len(set(cluster_labels)) &gt; 1 and -1 in cluster_labels:  # ignoring the noise\n            silhouette_avg = silhouette_score(yelp_zip, cluster_labels) # getting the silhouette scores\n            sil_scores.append(silhouette_avg)\n            if silhouette_avg &gt; max: # if the silhouette score is greater than the current max value\n                max = silhouette_avg # make this the new max\n                eps_and_sample.append((epsilon, sample)) # save the hyperparamter values\nprint(max)\nprint(eps_and_sample[-1]) # get only the hyperparamters for the max value\n\neps = 0.9 # from the results above\nmin_samples = 10\nclustering = DBSCAN(eps=eps, min_samples=min_samples) # initialize DBSCAN with optimal hyperparameters\ncluster_labels = clustering.fit_predict(yelp_zip) \n\nplt.scatter(yelp_zip['rating'], yelp_zip['review_count'], c=cluster_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Number of Reviews')\nplt.show()\n\n0.20967415255007046\n(1.0, 2)\n\n\n\n\n\n\n\n\n\nIn the above, we visualized the number of reviews against the yelp ratings utilizing DBSCAN clustering. The plot shows no clear clusters as the points of each color are dispersed across the yelp axis and the points are mostly noise. This indicates that there is no strong correlation between the number of reviews and the yelp rating.\n\n# DBSCAN rating and income\nplt.scatter(yelp_zip['rating'], yelp_zip['Median_Income'], c=cluster_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Median Income')\nplt.show()\n\n\n\n\n\n\n\n\nWe then used the same technique of DBSCAN to visualize the number of reviews against the median income. We find that the plot shows no clear clusters, dispersed points, and mostly points that would be classified as noise. This indicates that there is no strong correlation between the number of reviews and the surronding’s area median income.\n\n# DBSCAN rating and price\nplt.scatter(yelp_zip['rating'], yelp_zip['price_range'], c=cluster_labels, cmap='viridis')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Yelp Rating')\nplt.ylabel('Price Range')\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we tried to visualize the Yelp rating and the price range of the restaurant. We find that the plot shows no clear clusters as the points of each color are dispersed across the yelp axis. This indicates that there is no strong correlation between the Yelp rating and the price range of the restaurant.\n\n\nHierarchical Clustering\n\n# Hierarchical clustering\nlinkage_matrix = linkage(yelp_zip, method='ward') # creating linkage matrix\n\n# plotting the hierarchical clustering (utilized CHATGPT for help here)\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=yelp_zip.index, leaf_rotation=90, leaf_font_size=10)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()\n\nnum_clusters = 4 # from dendogram inspection\nclusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\nyelp_zip['Cluster'] = clusters\n\n\n\n\n\n\n\n\nIn the above plot, we visualized a dendogram utilizing hierarchical clustering on the Yelp data. The dendogram splits the data into 3 color clusters. The red and green clusters are the most similar because they have the smallest height of their joining link, however, this link is still relatively high as it is about 28 in terms of distance, indicating low similarity. The similarity between orange and the mix of green and red is even higher at about 35 on the distance axis. This seems to indicate that there are not any truly similar clusters and likely no natural clusters within the dataset."
  },
  {
    "objectID": "index.html#food-for-thought-investigating-the-makeup-of-a-yelp-rating-score",
    "href": "index.html#food-for-thought-investigating-the-makeup-of-a-yelp-rating-score",
    "title": "Landing page",
    "section": "",
<<<<<<< HEAD
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\n\n\n\n\n\n\n\n\n\n\n\nInclude your data science questions on this page.\n\nWhat factors are most important to a high Yelp Rating score?\n\n\nDo better neighborhoods (measured by median income) generally have better restaurants (measured by Yelp ratings)?\n\n\nAre certain types of restaurants more likely to be rated higher on yelp?\n\n\nDoes price of the restaurant factor into Yelp rating?\n\n\nIs it possible to predict the Yelp rating of a restaurant based on its features?\n\n\n\n\n\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\n\n\n\n\n\n\nCharacterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews by Daniel Keller and Maria Kostromitina\n\nKeller, D., & Kostromitina, M. (2020). Characterizing non-chain restaurants’ Yelp star-ratings: Generalizable findings from a representative sample of Yelp reviews. International Journal of Hospitality Management, 86, 102440.\n\n\nIn “Characterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews” Keller and Kostromitina attempt to investigate whether Yelp star ratings are characterized by different criteria, ie is there a particular reason that restaurants are given the particular rating and is there a consistent pattern across these restaurants. To achieve this goal, the authors took data from Yelp.com that totaled 54,000 reviews on restaurants that were categorized as non-chain restaurants. The dataset includes the Yelp rating of the restaurant, the reviews, and other metadata about the specific restaurant. They then employed multiple correspondance analysis (MCA) to investigate the underlying structures and patterns of the data. They found that the service, food quality, and the overall environment of a restaurant does, in fact, matter for the Yelp rating of a particular restaurant. Further, they found that this effect was varied across the different star ratings for Yelp. For the 1 to 2 star range, the authors found that the most important factor contributing to the rating was the wait time for food. For the 3 star establishments, the most important factor was the quality of the food and for the 4 to 5 star rating range the most important factor was the ability to cultivate a positive customer experience, which is a combination of the wait time, the food quality, and the service.\n\nDoes Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp by Chun Kuang\n\nKuang, C. (2017). Does quality matter in local consumption amenities? An empirical investigation with Yelp. Journal of Urban Economics, 100, 1-18.\n\n\nKuang (2017) attempted to look at whether the quality of consumption anemities (ie restaurants) have a significant impact on the surronding local economy. In the paper “Does Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp”, the author utilized housing data from the D.C. Office of Tax and Revenue Computer Assisted Mass Appraisal Database coupled with restaurant data collected from Yelp.com to investigate the effect on housing prices of quality restaurants. The quality of a restaurant was measured by consumer ratings and price estimates from Yelp. To investigate this guiding question, Kuang employed econometric techniques including regression analysis, neighborhood-year fixed effects, and difference-in-difference (DID) estimation. The DID estimation was done before and after the popularization of Yelp in the DC area across time periods and restaurant measures. With the DID, the author wanted to determine whether information on restaurant amenitieis matters and particularly what type of information matters. In addition, robustness checks and falsification were employed to see if the results were robust to a restricted housing sample, a different choice of radius, including other local attributes, and utilizing a smaller timeframe. The end result was that highly rated restaurants do, in fact, attract customers, which generates revenues, and positively impacts the local economy. Further, that Yelp reviews do act as a reliable signal of quality for a restaurant. They conclude that there is a positive effect on housing prices due to both the quantity and quality of restaurants in a given area, but caution this do not imply causation and building quality anemities will not increase housing prices.\n\nYelp Review Rating Prediction: Machine Learning and Deep Learning Models by Zefang Liu\n\nLiu, Z. (2020). Yelp review rating prediction: Machine learning and deep learning models. arXiv preprint arXiv:2012.06690.\n\n\nIn “Yelp Review Rating Prediction: Machine Learning and Deep Learning Models”, Liu attempts to predict the yelp rating score based on the cumulative sentiment of the Yelp reviews for restaurants. The author begins by utilizing the Yelp Open dataset, which provides data on the businesses that are listed on the website. This dataset is subsetted to include only restaurants, which leads to 63,944 review observations. The author notes that the dataset is highly skewed due to the restaurants with more reviews, generally having a higher rating (in the 4-5 star yelp rating range). From these reviews, two vectorizer techniques are employed, TF-IDF and a count vectorizer. The author determines that the TF-IDf is found to be a better vectorizer for the review text based on evaluation metrics. From here, four machine learning models are trained and testing on the TF-IDF data: Naive Bayes, logistic regression, random forest, and linear support vector machine. In addition, four transformer models are trained and tested on the text data: BERT, DistillBERT, XLNet, and RoBERTa. Evaluation metrics including accuracy, F1-score, and confusion matrics are utilized to evaluate all of the models. The end result is that the model are able to partially reliably predict the Yelp rating score. Particularly, there was foudn to be a 64% accuracy score for prediction using the Machine Learning techniques and a 70% accuracy score for rating prediction using the transformer techniques.\n\nAnalysis of Yelp Review by Peter Hajas, Louis Gutierrez, and Mukkai S. Krishnamoorthy\n\nHajas, P., Gutierrez, L., & Krishnamoorthy, M. S. (2014). Analysis of yelp reviews. arXiv preprint arXiv:1407.1443.\n\n\nThis article examines the role of reviews and ratings in predicting business success, focusing specifically on restaurants in college towns. The authors highlight the unique characteristics of college-town restaurants, noting their relatively short average lifespan of around four years. The study analyzes data from 20 different college campuses over a 7-year period and employs mathematical modeling, including differential equations, to understand Yelp reviews. Key findings include the observation that the relationship between the number of reviews and ratings follows a power-law distribution. Additionally, restaurants with a higher number of reviews tend to cluster geographically. The researchers also analyze the running average rating of all restaurants in a college town over time, alongside detailed case studies of the most-reviewed restaurant in each town. The study reveals that initial reviews can significantly impact a restaurant’s trajectory, but these effects stabilize as more reviews accumulate. They attribute some trends to college students’ unique dining habits. For example, pizza restaurants consistently rank highly and are the most common across towns, while university-sponsored dining establishments often receive lower ratings. Interestingly, the types of top-rated ethnic restaurants vary between towns, reflecting local cultural preferences.\n\nPredicting the Helpfulness of Online Restaurant Reviews Using Different Machine Learning Algorithms: A Case Study of Yelp by Yi Luo and Xiaowei Xu\n\nLuo, Y., & Xu, X. (2019). Predicting the helpfulness of online restaurant reviews using different machine learning algorithms: A case study of yelp. Sustainability, 11(19), 5254.\n\n\nThis 2019 article delves into the importance of Yelp reviews and their role in the restaurant industry, highlighting the challenges restaurants face, such as rising food prices, high labor costs, and a failure rate exceeding 60% within the first three years. It underscores the economic significance of the restaurant industry, noting its contribution to local economic growth and employment rates. The article also emphasizes the critical role of online reviews, citing that 94% of people choose a restaurant based on reviews. The study focuses on two key aspects for machine learning models: dining features of restaurants and customer reviews. It employs models like Naive Bayes and Naive Bayes combined with Support Vector Machines (SVM), with SVM achieving the highest F1 accuracy at 71%. The research identifies four essential features influencing customer satisfaction: taste, experience, value, and location. It observes that with an increasing number of reviews, the quality of food emerges as the most important factor for customers, followed by service. Negative reviews are primarily linked to concerns about price or value. This work explores the intersection of natural language processing (NLP) and machine learning in analyzing Yelp reviews to predict business success, offering valuable insights into customer preferences and the factors driving restaurant performance\n\nApplications of Machine Learning to Predict Yelp Ratings by Kyle Carbon, Kacyn Jujii, and Parasanth Veerina\n\nCarbon, K., Fujii, K., & Veerina, P. (2014). Applications of machine learning to predict Yelp ratings. 2014.\n\n\nThe study “Applications of Machine Learning to Predict Yelp Ratings” by Kyle Carbon, Kacyn Jujii, and Parasanth Veerina investigates factors influencing Yelp ratings and business performance, utilizing data from Phoenix, AZ. The authors employ K-means clustering to evaluate the role of location by measuring distances to shopping malls and popular landmarks. They conduct statistical tests to identify significant features and implement machine learning models, including logistic regression, SVM, random forests, and decision trees, achieving an average accuracy of 45%. The findings highlight that while factors like location, price range, and availability of take-out services significantly impact ratings, the most critical determinant is review sentiment. Notably, different features were found to influence ratings for specific business types; for example, speed is prioritized for fast-food establishments, whereas quality is emphasized for upscale restaurants. Sentiment classification emerged as the most predictive feature, indicating that customer review sentiment is the strongest indicator of business ratings on Yelp.\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
=======
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\n\n\n\n\n\n\n\n\n\n\n\nInclude your data science questions on this page.\n\nWhat factors are most important to a high Yelp Rating score?\n\n\nDo better neighborhoods (measured by median income) generally have better restaurants (measured by Yelp ratings)?\n\n\nAre certain types of restaurants more likely to be rated higher on yelp?\n\n\nDoes price of the restaurant factor into Yelp rating?\n\n\nIs it possible to predict the Yelp rating of a restaurant based on its features?\n\n\n\n\n\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\n\n\n\n\n\n\nCharacterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews by Daniel Keller and Maria Kostromitina\n\nKeller, D., & Kostromitina, M. (2020). Characterizing non-chain restaurants’ Yelp star-ratings: Generalizable findings from a representative sample of Yelp reviews. International Journal of Hospitality Management, 86, 102440.\n\n\nIn “Characterizing Non-Chain Restaurants’ Yelp Star-Ratings: Generalizable Findings from a Representative Sample of Yelp Reviews” Keller and Kostromitina attempt to investigate whether Yelp star ratings are characterized by different criteria, ie is there a particular reason that restaurants are given the particular rating and is there a consistent pattern across these restaurants. To achieve this goal, the authors took data from Yelp.com that totaled 54,000 reviews on restaurants that were categorized as non-chain restaurants. The dataset includes the Yelp rating of the restaurant, the reviews, and other metadata about the specific restaurant. They then employed multiple correspondance analysis (MCA) to investigate the underlying structures and patterns of the data. They found that the service, food quality, and the overall environment of a restaurant does, in fact, matter for the Yelp rating of a particular restaurant. Further, they found that this effect was varied across the different star ratings for Yelp. For the 1 to 2 star range, the authors found that the most important factor contributing to the rating was the wait time for food. For the 3 star establishments, the most important factor was the quality of the food and for the 4 to 5 star rating range the most important factor was the ability to cultivate a positive customer experience, which is a combination of the wait time, the food quality, and the service.\n\nDoes Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp by Chun Kuang\n\nKuang, C. (2017). Does quality matter in local consumption amenities? An empirical investigation with Yelp. Journal of Urban Economics, 100, 1-18.\n\n\nKuang (2017) attempted to look at whether the quality of consumption anemities (ie restaurants) have a significant impact on the surronding local economy. In the paper “Does Quality Matter in Local Consumption Amenities? An Empirical Investigation with Yelp”, the author utilized housing data from the D.C. Office of Tax and Revenue Computer Assisted Mass Appraisal Database coupled with restaurant data collected from Yelp.com to investigate the effect on housing prices of quality restaurants. The quality of a restaurant was measured by consumer ratings and price estimates from Yelp. To investigate this guiding question, Kuang employed econometric techniques including regression analysis, neighborhood-year fixed effects, and difference-in-difference (DID) estimation. The DID estimation was done before and after the popularization of Yelp in the DC area across time periods and restaurant measures. With the DID, the author wanted to determine whether information on restaurant amenitieis matters and particularly what type of information matters. In addition, robustness checks and falsification were employed to see if the results were robust to a restricted housing sample, a different choice of radius, including other local attributes, and utilizing a smaller timeframe. The end result was that highly rated restaurants do, in fact, attract customers, which generates revenues, and positively impacts the local economy. Further, that Yelp reviews do act as a reliable signal of quality for a restaurant. They conclude that there is a positive effect on housing prices due to both the quantity and quality of restaurants in a given area, but caution this do not imply causation and building quality anemities will not increase housing prices.\n\nYelp Review Rating Prediction: Machine Learning and Deep Learning Models by Zefang Liu\n\nLiu, Z. (2020). Yelp review rating prediction: Machine learning and deep learning models. arXiv preprint arXiv:2012.06690.\n\n\nIn “Yelp Review Rating Prediction: Machine Learning and Deep Learning Models”, Liu attempts to predict the yelp rating score based on the cumulative sentiment of the Yelp reviews for restaurants. The author begins by utilizing the Yelp Open dataset, which provides data on the businesses that are listed on the website. This dataset is subsetted to include only restaurants, which leads to 63,944 review observations. The author notes that the dataset is highly skewed due to the restaurants with more reviews, generally having a higher rating (in the 4-5 star yelp rating range). From these reviews, two vectorizer techniques are employed, TF-IDF and a count vectorizer. The author determines that the TF-IDf is found to be a better vectorizer for the review text based on evaluation metrics. From here, four machine learning models are trained and testing on the TF-IDF data: Naive Bayes, logistic regression, random forest, and linear support vector machine. In addition, four transformer models are trained and tested on the text data: BERT, DistillBERT, XLNet, and RoBERTa. Evaluation metrics including accuracy, F1-score, and confusion matrics are utilized to evaluate all of the models. The end result is that the model are able to partially reliably predict the Yelp rating score. Particularly, there was foudn to be a 64% accuracy score for prediction using the Machine Learning techniques and a 70% accuracy score for rating prediction using the transformer techniques.\n\nAnalysis of Yelp Review by Peter Hajas, Louis Gutierrez, and Mukkai S. Krishnamoorthy\nPredicting the Helpfulness of Online Restaurant Reviews Using Different Machine Learning Algorithms: A Case Study of Yelp by Yi Luo and Xiaowei Xu\nApplications of Machine Learning to Predict Yelp Ratings by Kyle Carbon, Kacyn Jujii, and Parasanth Veerina #added\n\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code-mostly-using-github-copilot-to-work-off-own-code-but-some-chatgpt-usage",
    "href": "technical-details/llm-usage-log.html#code-mostly-using-github-copilot-to-work-off-own-code-but-some-chatgpt-usage",
    "title": "LLM usage log",
    "section": "Code (Mostly using GitHub Copilot to work off own code, but some ChatGPT usage):",
    "text": "Code (Mostly using GitHub Copilot to work off own code, but some ChatGPT usage):\n\nCode commenting and explanatory documentation\nScatter plot for the missing values (getting the hue to show missing values)\nOutlier detection using z-score calculation\nPie chart plotting\nCorrelation matrix calcultion and heatmap plot\nCrosstabulation calculations\nANOVA testing\n3D interactive plot for K-Means\nHierarchical clustering"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#html",
    "href": "technical-details/llm-usage-log.html#html",
    "title": "LLM usage log",
    "section": "HTML:",
    "text": "HTML:\n\nHelping set up and reformat HTML code from lab 1.1 to create a working About Me page"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#other-citations-these-are-not-llm-but-references",
    "href": "technical-details/llm-usage-log.html#other-citations-these-are-not-llm-but-references",
    "title": "LLM usage log",
    "section": "Other Citations (these are not LLM, but references):",
    "text": "Other Citations (these are not LLM, but references):\n\nLab 1.1 for About Me HTML\nLab 4.1 for kmean_fit function and help with executing k-Means and Silhouette score\nLab 4.1 for DBSCAN execution\nLab 4.2 for plot_variance_explained function\nLab 4.2 for plot_2D function"
>>>>>>> 3781182d5eb1262e0992a51f6e4d12351a242c28
  }
]